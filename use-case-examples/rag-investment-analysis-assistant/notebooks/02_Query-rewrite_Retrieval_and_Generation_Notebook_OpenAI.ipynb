{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b779787-fb64-48da-be54-9318ed3ee5f3",
   "metadata": {},
   "source": [
    "# Investment Analyst Assistant Retreival Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3c9f6",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "Copyright 2024 Amazon Web Services, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342aafd-8be8-4d34-af97-a7723514392d",
   "metadata": {},
   "source": [
    "##### This notebook allows you to generate metadata forom the question. This metadata will be used to retreived specific chunks from the Opensearch Index which is then sent to the LLM to generate answer for the specific question. This notebook is configured to use \"OpenAI\"GPT models.\n",
    "#### In case you do not have OpenAI API Key/ Access; You can use mistral AI available on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265c919-8e10-487e-98b5-e28a049e1533",
   "metadata": {},
   "source": [
    "### STEP 0:  Reset And Install missing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733402c0",
   "metadata": {},
   "source": [
    "NOTE: Warnings and in some case, version errors can be ignored for package installation. Those are due to version updates. Only change versions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a79159b-b97b-4679-8a45-e874e51fba26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4997921f-b4e9-4e38-a66a-76576722e6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-scheduler 2.7.1 requires pydantic<3,>=1.10, but you have pydantic 1.9.2 which is incompatible.\n",
      "langchain 0.1.9 requires langchain-core<0.2,>=0.1.26, but you have langchain-core 0.2.41 which is incompatible.\n",
      "langchain-text-splitters 0.3.0 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
      "ragas 0.0.0 requires boto3==1.33.9, but you have boto3 1.34.162 which is incompatible.\n",
      "ragas 0.0.0 requires botocore==1.33.9, but you have botocore 1.34.162 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-text-splitters 0.3.0 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 0.1.52 which is incompatible.\n",
      "ragas 0.0.0 requires boto3==1.33.9, but you have boto3 1.34.162 which is incompatible.\n",
      "ragas 0.0.0 requires botocore==1.33.9, but you have botocore 1.34.162 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!python -m pip install --upgrade pip --quiet\n",
    "!pip install requests_toolbelt --quiet\n",
    "# !pip install transformers --quiet  #install only if necessary\n",
    "# !pip install llama_index --quiet   #install only if necessary\n",
    "!pip install requests_aws4auth --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install --upgrade openai --quiet\n",
    "!pip install \"pydantic>=1.8.2,<1.10.0\" --quiet\n",
    "\n",
    "!pip install langchain==0.1.9 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929dc4d5",
   "metadata": {},
   "source": [
    "#### Adding Project Directory to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c092feaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust this path as needed\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab5e84-fdb5-4942-b769-546b5133da06",
   "metadata": {},
   "source": [
    "### STEP 1: Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45604a88-6167-426e-8bfe-de692af0214f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from libraries.iaa.experiments.utils_exp import get_titan_text_embedding, results_fusion\n",
    "from libraries.iaa.query_transformation.ExtractQueryMetadata import ExtractQueryMetadata\n",
    "from libraries.iaa.reranker.SearchRanker import SearchRanker\n",
    "from libraries.iaa.retrieval.OpenSearchRetrieval import OpenSearchRetrieval\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from libraries.iaa.query_transformation.QueryMetaExtractor import QueryMetaExtractor\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import libraries.iaa.configs as configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d8202-a25b-4b90-82d7-89c8729ac9ab",
   "metadata": {},
   "source": [
    "### STEP 2: Notebook Configuration\n",
    "\n",
    "#### Users Please choose the LLM from the options provided.\n",
    "#### If users are looking to experiment with GPT models, Please add you API key in the field indicated.\n",
    "#### If you DO NOT have OpenAI Key, Please use 'mistral.mistral-large-2402-v1:0' for 'us-west-2'(Default for this workshop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec4ccd3-a61a-40fc-bcd9-fa6684f9bc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_config = {\n",
    "    'type_rets': 'fusion', #fusion (text and embb) is selected for this workshop because it generated the best results\n",
    "    'opensearch_host': configs.OPEN_SEARCH_HOST, # copy the url created in the index creation notebook\n",
    "    'generation_llm_model': 'mistral.mistral-large-2402-v1:0', #mistral.mistral-small-2402-v1:0  or gpt-4-turbo or gpt-3.5-turbo or gpt-XX-turbo or mistral.mistral-large-2402-v1:0 (for us-west-2)\n",
    "    #'OPENAI_API_KEY': '----***----', #Use your OpenAI API Key\n",
    "    #'AZURE_OPENAI_API_KEY': '----***----', #Use your Azure OpenAI API Key\n",
    "    'index_name_embb': 'expt_index', #name of the index created in the index creation notebook\n",
    "    'index_name_text': 'expt_index', #name of the index created in the index creation notebook\n",
    "    'top_k_ranking': 10,\n",
    "    'top_k_retrieval': 20,\n",
    "    'emb_name': 'vector_field',\n",
    "    'region': configs.REGION\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0524f9-dc7a-485b-b48a-b4fd485bd889",
   "metadata": {},
   "source": [
    "#### Azure OpenAI Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c3d8d-73c2-4ea6-810c-f0a10fe246c8",
   "metadata": {},
   "source": [
    "Configuration if choose to use Azure OpenAI over OpenAI. Default is set to use OpenAI to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6caea05-5e68-40ec-9dcc-d27e5a203568",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_azure_openai = False\n",
    "\n",
    "if invoke_azure_openai == True:\n",
    "    # Download the certificate\n",
    "    !curl -o ca-bundle-full.crt https://link-to-certificate\n",
    "    # Set the SSL certificate file environment variable\n",
    "    os.environ['SSL_CERT_FILE'] = 'ca-bundle-full.crt' # Add path to the certificate file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89410f09-8bff-4246-8bf2-9b1d8007ce8e",
   "metadata": {},
   "source": [
    "#### Input Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ada23c-07fe-48fa-9a93-eac13d825d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What was the revenue for 3M in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723dc8c-cd8c-44f3-a852-831acdb7c89d",
   "metadata": {},
   "source": [
    "### STEP 3 : Rephrase And Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d79a2-4046-488b-bbfe-1330e4c2bfe9",
   "metadata": {},
   "source": [
    "#### Prompt to Extract Metadata from the Quesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e0d30a-7759-4dbb-b773-dc5b0cadf367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_METADATA_GENERATION_time = \"\"\"\n",
    "You are a financial editor responsible for rephrasing user questions accurately for better search and retrieval tasks related to yearly and quarterly financial reports. The current year is {most_recent_year}, and the current quarter is {most_recent_quarter}.\n",
    "Task: Given a user question:{query}, identify the following metadata as per the instructions below:\n",
    "1. time_keyword_type: Identifies what type of time range the user is requesting - a range of years, a range of quarters, specific years, specific quarters, or none.\n",
    "2. time_keywords: If time_keyword_type is \"range of periods,\" these keywords expand the year or quarter period. Otherwise, it will be the formatted version of the year in YYYY format or the quarter in Q'YY format.\n",
    "Instructions:\n",
    "1. Identify whether the user is asking for a date range or specific set of years or quarters. If there is no year or quarter mentioned, leave time_keyword blank.\n",
    "2. If the user is requesting specific years, return the year(s) in YYYY format.\n",
    "3. If the user is requesting specific quarters, return the quarter(s) in Q'YY format (e.g., Q2'24, Q1'23).\n",
    "4. If the user is requesting documents within a specific time range between two periods, fill in the year or quarter information between the time ranges.\n",
    "5. If the user is requesting the last N years, count backward from the current year, {most_recent_year}.\n",
    "6. If the user is requesting the last N quarters, count backward from the current quarter and year, {most_recent_quarter}.\n",
    "Examples:\n",
    "what was Google's net profit?\n",
    "time_keyword_type: none\n",
    "time_keywords: none\n",
    "explanation: no quarter or year mentioned\n",
    "What was Amazon's total sales in 2022?\n",
    "time_keyword_type: specific_year\n",
    "time_keywords: 2022\n",
    "What was Apple's revenue in 2019 compared to 2018?\n",
    "time_keyword_type: specific_year\n",
    "time_keywords: 2018, 2019\n",
    "explanation: the user is requesting to compare 2 different years\n",
    "Which of Disney's business segments had the highest growth in sales in Q4 F2023?\n",
    "time_keyword_type: specific_quarter\n",
    "time_keywords: Q4 2023\n",
    "How did Netflix's quarterly spending on research change as a percentage of quarterly revenue change between Q2 2019 and Q4 2019?\n",
    "time_keyword_type: range_quarter\n",
    "time_keywords: Q2 2019, Q3 2019, Q4 2019\n",
    "explanation: the quarters between Q2 2019 and Q4 2019 are Q2 2019, Q3 2019 and Q4 2019\n",
    "What was Spotify's growth in the last 5 quarters?\n",
    "time_keyword_type: range_quarter\n",
    "time_keywords: Q4 2023, Q3 2023, Q2 2023, Q1 2023, Q4 2024\n",
    "explanation: Since the current quarter is Q1 2024, the last 5 quarters are Q4 2023, Q3 2023, Q2 2023, Q1 2023, and Q4 2024.\n",
    "In their 10-K filings, has Norwegian Cruise mentioned any negative environmental or weather-related impacts to their business in the last four years?\n",
    "time_keyword_type: range_year\n",
    "time_keywords: 2020, 2021, 2022, 2023\n",
    "explanation: Since the current year is {most_recent_year}, the last four years are 2020, 2021, 2022, and 2023.\n",
    "Return a JSON object with the following fields:\n",
    "- 'time_keyword_type': The type of time range the user is requesting.\n",
    "- 'time_keywords': The specific time-related keywords identified in the user's question.\n",
    "- 'explanation': An explanation of why you chose a certain time_keyword_type and time_keywords.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23af28ce-f6ef-48d4-8c76-af8b1d1dd257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_METADATA_TECHNICAL_KWD = \"\"\"\n",
    "\n",
    "Imagine you are a financial analyst looking to answer the question: {query}\n",
    "Your task is to generate a list of 5-6 important keywords that you would use for searching relevant sections in companies 10-K and 10-Q documents to find information related to the given question.\n",
    "Instructions:\n",
    "1. Do not include company names, document names, or timelines in the keywords.\n",
    "2. Generate a list of 5-6 comma-separated keywords.\n",
    "3. Focus on identifying the sections of the documents you would look at, and include those section names or topics in the keywords.\n",
    "4. Do not add keywords that are not part of or directly related to the given question.\n",
    "Your response should be a comma-separated list of keywords without any additional formatting or tags.\n",
    "For example, if the question is 'What was Google's net profit?', a possible response could be:\n",
    "net profit, income statement, revenues, expenses, earnings \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86fd46bb-364e-499b-8af1-25d71de36fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_METADATA_AND_QUERY_ReWr = \"\"\"\n",
    "Human: You are a financial editor that looks at a user question and rephrases it accurately for better search and retrieval tasks. The question is related to yearly and quarterly financial reports.\n",
    "Task: Given a user question, identify the following metadata: {query}\n",
    "Technical Keywords: Provide a list of relevant keywords extracted from the question that are typically found in financial documents.\n",
    "Generate a comprehensive list of all possible keywords relevant to financial sections.\n",
    "Include different alternatives and variations of these keywords.\n",
    "Exclude company names and document titles from this list.\n",
    "Company Keywords: Extract a list of company names mentioned in the question.\n",
    "Rephrased Question: Rephrase the question to make it clearer.\n",
    "Expand any acronyms or abbreviations found in the original question, including both the abbreviated and expanded versions.\n",
    "Make the rephrased question as clear as possible.\n",
    "Output: Return a JSON object with the following fields:\n",
    "'technical_keywords': A list of relevant keywords from the question.\n",
    "'company_keywords': A list of company names mentioned in the question.\n",
    "'rephrased_question': The fully rephrased question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad613b15-8da0-43bc-9b3f-c932c5b67837",
   "metadata": {},
   "source": [
    "#### Metadata Generation Using prompts and OpenAI GPT llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f106f506-edc9-4f6f-8da4-0280129d6543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_azure_openai_response(prompt, model_id, token_count=512, temperature=0, topP=1):\n",
    "    client = AzureOpenAI(\n",
    "                api_version=\"2024-02-01\",\n",
    "                azure_endpoint=\"https://your-azure-openai-endpoint\",\n",
    "                api_key=exp_config['AZURE_OPENAI_API_KEY']\n",
    "                )\n",
    "    gpt_assistant_prompt = \"You a financial analyst that looks at a user question, related time and technical keywords and potentially relevant context.\"\n",
    "    message=[{\"role\": \"system\", \"content\": gpt_assistant_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model = str(model_id),\n",
    "        # deployment_id= str(model_id),\n",
    "        messages=message,\n",
    "        temperature=temperature,\n",
    "        max_tokens=token_count,\n",
    "    )\n",
    "    input_token_str = str(response.usage)\n",
    "    # Find the index of the start of the completion_tokens value\n",
    "    start_index = input_token_str.find(\"completion_tokens=\") + len(\"completion_tokens=\")\n",
    "    end_index = input_token_str.find(\",\", start_index)\n",
    "    output_tokens = int(input_token_str[start_index:end_index])\n",
    "\n",
    "    # Find the index of the start of the prompt_tokens value\n",
    "    start_index = input_token_str.find(\"prompt_tokens=\") + len(\"prompt_tokens=\")\n",
    "    end_index = input_token_str.find(\",\", start_index)\n",
    "    input_tokens = int(input_token_str[start_index:end_index])\n",
    "    response_final = {'resp':response.choices[0].message.content, 'input':input_tokens, 'output':output_tokens}\n",
    "    return response_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90634fa5-ad86-44aa-a474-6d7a9bf390ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_openai_response(prompt, model_id, token_count=512, temperature=0, topP=1):\n",
    "    os.environ['OPENAI_API_KEY'] = exp_config['OPENAI_API_KEY']\n",
    "    client = OpenAI()\n",
    "    gpt_assistant_prompt = \"You a financial analyst that looks at a user question, related time and technical keywords and potentially relevant context.\"\n",
    "    message=[{\"role\": \"system\", \"content\": gpt_assistant_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model= str(model_id),\n",
    "        messages=message,\n",
    "        temperature=temperature,\n",
    "        max_tokens=token_count,\n",
    "    )\n",
    "    input_token_str = str(response.usage)\n",
    "    # Find the index of the start of the completion_tokens value\n",
    "    start_index = input_token_str.find(\"completion_tokens=\") + len(\"completion_tokens=\")\n",
    "    end_index = input_token_str.find(\",\", start_index)\n",
    "    output_tokens = int(input_token_str[start_index:end_index])\n",
    "\n",
    "    # Find the index of the start of the prompt_tokens value\n",
    "    start_index = input_token_str.find(\"prompt_tokens=\") + len(\"prompt_tokens=\")\n",
    "    end_index = input_token_str.find(\",\", start_index)\n",
    "    input_tokens = int(input_token_str[start_index:end_index])\n",
    "    response_final = {'resp':response.choices[0].message.content, 'input':input_tokens, 'output':output_tokens}\n",
    "    return response_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d8878-bef3-499a-bf28-a2b6b6dc3b81",
   "metadata": {},
   "source": [
    "#### Metadata Generation Using prompts and MistralAI llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1062d2e9-6747-44ca-a951-eedde5be7054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mistralai_response(\n",
    "    prompt,\n",
    "    model_id,\n",
    "    token_count=500,\n",
    "    temp=0,\n",
    "    topP=1,\n",
    "    max_tokens: int = 800,\n",
    "    temperature: float = 0):\n",
    "    inference_config = {\n",
    "        \"maxTokens\": token_count,\n",
    "        \"temperature\": temp,\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": prompt + \"Do not show the steps or examples, only give the answer specific to the question\"}]},\n",
    "    ]\n",
    "\n",
    "    bedrock_client = boto3.client(\"bedrock-runtime\", region_name=exp_config['region'])\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "    output = response['output']['message']['content'][0]['text']\n",
    "    input_tokens = int(response['usage']['inputTokens'])\n",
    "    output_tokens = int(response['usage']['outputTokens'])\n",
    "    response_final = {'resp':output, 'input':input_tokens, 'output':output_tokens}\n",
    "    return response_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead950c1-789e-4283-ba72-6547303cb2a3",
   "metadata": {},
   "source": [
    "#### LLM Selection Based on config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60224ae7-8aaa-4a85-b7e1-0d6d4ccfc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(model_id, prompt, invoke_azure_openai):\n",
    "    if \"gpt\" in model_id:\n",
    "        if not invoke_azure_openai:\n",
    "            response = get_openai_response(prompt, model_id, token_count=512, temperature=0, topP=0.5)\n",
    "        else:\n",
    "            response = get_azure_openai_response(prompt, model_id, token_count=512, temperature=0, topP=0.5)\n",
    "    else:\n",
    "        response = get_mistralai_response(prompt, model_id, token_count=512, temperature=0, topP=0.5)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b591165-5c37-4bcf-97a6-d35957569a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Function to get the LLM response in desired format\n",
    "def llm_ouput_to_json_time(llm_output):\n",
    "    # Use regular expressions to find content between curly braces\n",
    "    pattern = r\"\\{([^}]*)\\}\"\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "    if len(matches) < 1:\n",
    "        return \"\", []\n",
    "    try:\n",
    "        json_obj = json.loads(\"{\" + matches[0] + \"}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {str(e)}\")\n",
    "        return \"\", []\n",
    "    return (json_obj[\"time_keyword_type\"], json_obj[\"time_keywords\"], json_obj[\"explanation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f225594-c8d9-4f8e-99e5-ca42a3d9f2b7",
   "metadata": {},
   "source": [
    "#### Time Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07773d13-ada8-4f66-b97d-f335966a4624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_keyword_extraction(prompt, question):\n",
    "    prompt_format = prompt.format(\n",
    "        query=question, most_recent_quarter=\"Q1'24\", most_recent_year=2024)\n",
    "    response = get_llm_response(model_id = exp_config['generation_llm_model'],prompt=prompt_format, invoke_azure_openai= invoke_azure_openai)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19136fa3-cad2-4a12-b3e0-b8d3bd1c5464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time keywords generated: ('specific_year', '2022', 'The user is requesting the revenue for 3M in a specific year, which is 2022.')\n",
      "Total Input Token: 1037\n",
      "Total Output Token: 64\n"
     ]
    }
   ],
   "source": [
    "response = time_keyword_extraction(prompt=PROMPT_METADATA_GENERATION_time, question=question)\n",
    "time_keyword_type, time_kwds, explanation = llm_ouput_to_json_time(response['resp'])\n",
    "print(f\"Time keywords generated: {time_keyword_type, time_kwds, explanation}\")\n",
    "\n",
    "print(f\"Total Input Token: {response['input']}\")\n",
    "print(f\"Total Output Token: {response['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619a7193-d396-4a26-8e63-89ab463c68ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Function to get the LLM response in desired format\n",
    "def llm_output_kwd(llm_output):\n",
    "    llm_output_list = [llm_output]\n",
    "    keywords = llm_output_list\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a6d99-48b1-46e4-94de-0576ee4d9e8b",
   "metadata": {},
   "source": [
    "#### Technical Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfa5b2df-33e6-437b-bbc6-f45c8d18dc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Revenue, Consolidated Statements of Income, Total Sales, Operating Income, Gross Income, Income Statement']\n",
      "Total Input Token: 245\n",
      "Total Output Token: 29\n"
     ]
    }
   ],
   "source": [
    "def technical_keyword_extraction(prompt, question):\n",
    "    prompt_format = prompt.format(query=question)\n",
    "\n",
    "    response = get_llm_response(model_id = exp_config['generation_llm_model'],prompt=prompt_format, invoke_azure_openai= invoke_azure_openai)\n",
    "    return response\n",
    "response = technical_keyword_extraction(prompt=PROMPT_METADATA_TECHNICAL_KWD, question=question)\n",
    "kwds = llm_output_kwd(response['resp'])\n",
    "print(kwds)\n",
    "\n",
    "print(f\"Total Input Token: {response['input']}\")\n",
    "print(f\"Total Output Token: {response['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "377a47d9-3149-434e-aeef-c04a2dfe408f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Functions to get the LLM response in desired format\n",
    "def llm_ouput_to_json(llm_output):\n",
    "    # Use regular expressions to find content between curly braces\n",
    "    pattern = r\"\\{([^}]*)\\}\"\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "    if len(matches) < 1:\n",
    "        return \"\", []\n",
    "    try:\n",
    "        json_obj = json.loads(\"{\" + matches[0] + \"}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {str(e)}\")\n",
    "        return \"\", []\n",
    "    return (\n",
    "        json_obj[\"rephrased_question\"],\n",
    "        json_obj[\"technical_keywords\"],\n",
    "        json_obj[\"company_keywords\"] if \"company_keywords\" in json_obj.keys() else [],\n",
    "    )\n",
    "\n",
    "def convert_quarter_format(input_string):\n",
    "    # Ensure both quarter formats are included eg: Q2'22 <--> Q2 2022\n",
    "\n",
    "    # Use regular expression to match \"Q<quarter>'<year>\"\n",
    "    match = re.match(r\"Q(\\d)\\'(\\d{2})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        quarter = match.group(1)\n",
    "        year = \"20\" + match.group(2)\n",
    "        return f\"Q{quarter} {year}\"\n",
    "\n",
    "    match = re.match(r\"Q(\\d) (\\d{4})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        quarter = match.group(1)\n",
    "        year = match.group(2)[-2:]\n",
    "        return f\"Q{quarter}'{year}\"\n",
    "\n",
    "    # For case of e.g., Q1 F2023\n",
    "    match = re.match(r\"Q(\\d) F(\\d{4})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        quarter = match.group(1)\n",
    "        year = match.group(2)[-2:]\n",
    "        return f\"Q{quarter}'{year}\"\n",
    "\n",
    "    # For case of e.g., F2023\n",
    "    match = re.match(r\"F(\\d{4})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        return f\"{year}\"\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78116dca-10f9-429a-a1d0-c1b4c51f8da0",
   "metadata": {},
   "source": [
    "#### Combined Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f577bd70-d88a-4dea-9c5f-4f27a555520c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3M'] ['2022'] ['turnover', 'yearly', 'gross income', 'annual', 'net income', 'sales', 'financial report', '2022', 'income', 'earnings', 'revenue'] What was the total annual revenue reported by 3M (MMM) in the financial year of 2022?\n",
      "Total Input Token: 294\n",
      "Total Output Token: 110\n"
     ]
    }
   ],
   "source": [
    "def metadata_extraction(prompt, question, time_kwds):\n",
    "    prompt_format = prompt.format(query=question, most_recent_quarter=\"Q1'24\", time_kwds=time_kwds)\n",
    "\n",
    "    response = get_llm_response(model_id = exp_config['generation_llm_model'],prompt=prompt_format, invoke_azure_openai= invoke_azure_openai)\n",
    "    llm_output = response['resp']\n",
    "    rephrased_q, kwds, company_keywords = llm_ouput_to_json(llm_output)\n",
    "    time_kwds = time_kwds.split(\",\") if type(time_kwds) == str else time_kwds\n",
    "    time_kwds = [time_kwd.strip() for time_kwd in time_kwds]\n",
    "\n",
    "    new_time_kwds = []\n",
    "    for time_kwd in time_kwds:\n",
    "        new_time_kwd = convert_quarter_format(time_kwd)\n",
    "        if new_time_kwd:\n",
    "            new_time_kwds.append(new_time_kwd)\n",
    "    time_kwds.extend(new_time_kwds)\n",
    "    time_kwds = list(set(time_kwds))\n",
    "\n",
    "    kwds = list(set(kwds))\n",
    "    doc_type = []\n",
    "    if len(time_kwds) == 0 or time_kwds == [\"\"] or time_kwds == \"none\" or time_kwds is None:\n",
    "        time_kwds = [2022,2023,2024]\n",
    "    return company_keywords, time_kwds, kwds , rephrased_q, response\n",
    "\n",
    "company_keywords, time_kwds, kwds, rephrased_q, response = metadata_extraction(prompt=PROMPT_METADATA_AND_QUERY_ReWr, question =question, time_kwds=time_kwds)\n",
    "print(company_keywords, time_kwds, kwds, rephrased_q)\n",
    "print(f\"Total Input Token: {response['input']}\")\n",
    "print(f\"Total Output Token: {response['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8cfc4d3-1a9b-404f-a62c-cd19973e5348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Functions to get the LLM response in desired format\n",
    "def parse_time_kwds(time_kwds) -> list:\n",
    "    \"\"\"Given LLM generated time keywords, extract quarter and year\n",
    "\n",
    "    Args:\n",
    "        time_kwds (list): LLM generated time keywords\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with year and quarter\n",
    "    \"\"\"\n",
    "    set_tuple = set([])\n",
    "    for kwd in time_kwds:\n",
    "        match = re.match(r\"Q(\\d)\\'(\\d{2})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"q\" + match.group(1)\n",
    "            year = \"20\" + match.group(2)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "        match = re.match(r\"Q(\\d) (\\d{4})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"q\" + match.group(1)\n",
    "            year = match.group(2)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "        match = re.match(r\"Q(\\d) F(\\d{4})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"q\" + match.group(1)\n",
    "            year = match.group(2)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "        match = re.search(r\"(20\\d{2})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"\"\n",
    "            year = match.group(1)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "    return list(set_tuple)\n",
    "\n",
    "def get_list_variables(company_kwds, time_kwds, kwds):\n",
    "        time_kwds_tuples = parse_time_kwds(time_kwds)\n",
    "        time_key_in_years = list(set([x[0] for x in time_kwds_tuples]))\n",
    "        time_count = len(time_key_in_years)\n",
    "        q_kwds = []\n",
    "\n",
    "        return time_key_in_years, q_kwds\n",
    "\n",
    "time_key_in_years, q_kwds = get_list_variables(company_keywords, time_kwds, kwds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1a185-a315-40fc-938a-71dce76be4a3",
   "metadata": {},
   "source": [
    "#### Chunks Retreival from Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58baca5c-989b-4272-a6e2-ad39d585c584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_context(q, q_kwds, time_keyword_type, time_key_in_years, kwds, time_kwds, company_kwds):\n",
    "    doc_type = []\n",
    "    contexts = []\n",
    "    retriever_embb = OpenSearchRetrieval(exp_config['opensearch_host'], exp_config['index_name_embb'])\n",
    "    embedding = get_titan_text_embedding(q)\n",
    "    contexts_sem = retriever_embb.retrieve_semantic(\n",
    "        exp_config['emb_name'],\n",
    "        embedding,\n",
    "        time_kwds,\n",
    "        company_kwds,\n",
    "        doc_type,\n",
    "        q_kwds,\n",
    "        top_k=exp_config['top_k_retrieval'],\n",
    "        use_company_kwds=True,\n",
    "        use_doc_type=False,\n",
    "    )\n",
    "\n",
    "    retriever_text = OpenSearchRetrieval(exp_config['opensearch_host'], exp_config['index_name_embb'])\n",
    "    contexts_text = retriever_text.retrieve_text(\n",
    "        kwds,\n",
    "        time_kwds,\n",
    "        company_kwds,\n",
    "        doc_type,\n",
    "        exp_config['top_k_retrieval'],\n",
    "        True,\n",
    "        False,\n",
    "    )\n",
    "    contexts.extend(\n",
    "        [\n",
    "            context[\"paragraph\"]\n",
    "            for context in results_fusion([contexts_sem, contexts_text], [0.7, 0.3], top_k=exp_config['top_k_retrieval'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd4faeb-87b5-486f-8f7f-bc33c7db6462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using exact matching in semantic search with ['2022'], ['3M']\n",
      "Using exact matching in text search with ['2022'], ['3M']\n"
     ]
    }
   ],
   "source": [
    "contexts = get_context( rephrased_q, q_kwds, time_keyword_type, time_key_in_years, kwds, time_kwds, company_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ecc81-4625-4c30-9915-04dcc3b837f7",
   "metadata": {},
   "source": [
    "### Rerank the Retreived Chunks\n",
    "\n",
    "##### Keyword Reranker used for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b37efb24-cf23-462e-abef-526441346ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rerank_context(rephrased_query, kwds_list, time_kwds_list, company_kwds_list, contexts):\n",
    "    # Set the top k ranking based on time keywords\n",
    "    t0 = time.time()\n",
    "    print(\"With keyword reranker\")\n",
    "    # Rank the contexts\n",
    "    ranker = SearchRanker()\n",
    "    combined_kwds = []\n",
    "    combined_kwds.extend(set(kwds_list))\n",
    "    combined_kwds.extend(set(time_kwds_list))\n",
    "    combined_kwds.extend(set(company_kwds_list))\n",
    "    ranked_contexts = ranker.rank_by_word_frequency(combined_kwds, contexts)\n",
    "    top_k_contexts = ranked_contexts[: exp_config['top_k_ranking']]\n",
    "    end_time = time.time()\n",
    "    print(f\"**** Time taken for reranking {end_time - t0}\")\n",
    "    return ranked_contexts, top_k_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d53e9c2-4758-470d-bd54-230533190598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0024771690368652344\n"
     ]
    }
   ],
   "source": [
    "ranked_contexts, top_k_contexts = rerank_context(rephrased_q, q_kwds,time_kwds, company_keywords, contexts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61dde52f-619d-447b-8cf0-b383f500116f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks retreived = 20\n",
      "Number of Ranked Chunks to be sent to llm = 10\n"
     ]
    }
   ],
   "source": [
    "#Check point to see if contexts/chunks are retreived\n",
    "print (f\"Number of Chunks retreived = {len(ranked_contexts)}\")\n",
    "print (f\"Number of Ranked Chunks to be sent to llm = {len(top_k_contexts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891bef1-bef6-4d3d-9a61-2f2e0adc8edf",
   "metadata": {},
   "source": [
    "#### Prompt to generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eac0bdf-038d-42de-b708-403587b32ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_ANS_GENERATION  = \"\"\"\n",
    "To answer the financial question, think step-by-step:\n",
    "1. Carefully read the question and any provided context paragraphs related to yearly and quarterly document reports to find all relevant paragraphs. Prioritize context paragraphs with CSV tables.\n",
    "2. If needed, analyze financial trends and quarter-over-quarter (Q/Q) performance over the detected time spans mentioned in the related time keywords. Calculate rates of change between quarters to identify growth or decline.\n",
    "3. Perform any required calculations to get the final answer, such as sums or divisions. Show the math steps.\n",
    "4. Provide a complete, correct answer based on the given information. If information is missing, state what is needed to answer the question fully.\n",
    "5. Present numerical values in rounded format using easy-to-read units.\n",
    "6. Do not preface the answer with \"Based on the provided context\" or anything similar. Just provide the answer directly.\n",
    "7. Include the answer with relevant and exhaustive information across all contexts. Substantiate your answer with explanations grounded in the provided context. Conclude with a precise, concise, honest, and to-the-point answer.\n",
    "8. Add the page source and number.\n",
    "9. Add all source files from where the contexts were used to generate the answers.\n",
    "context = {CONTEXT}\n",
    "query = {QUERY}\n",
    "rephrased_query = {REPHARSED_QUERY}\n",
    "time_kwds = {TIME_KWDS}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05933ebb-8d42-47c1-b965-b4f86fd48f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans_gen_prompt = PROMPT_ANS_GENERATION.format(\n",
    "            QUERY=question,\n",
    "            CONTEXT=top_k_contexts,\n",
    "            TIME_KWDS=time_kwds,\n",
    "            REPHARSED_QUERY=rephrased_q,\n",
    "            most_recent_quarter=\"Q1'24\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3349d04-8d73-4a29-bb4a-2d110d56890d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answer Generation Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e71232c-66d6-4b04-abd9-60d47a03f47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Function to get the LLM response in desired format\n",
    "def parse_generation(llm_prediction):\n",
    "    soup = BeautifulSoup(llm_prediction, \"html.parser\")\n",
    "    answer = soup.find(\"answer\".lower())\n",
    "    page_source = soup.find(\"pages\".lower())\n",
    "    source = soup.find(\"src\".lower())\n",
    "\n",
    "    answer = re.sub(\"<[^<]+>\", \"\", str(answer))\n",
    "    page_source = re.sub(\"<[^<]+>\", \"\", str(page_source))\n",
    "    source = re.sub(\"<[^<]+>\", \"\", str(source))\n",
    "\n",
    "    return answer+'\\n'+page_source, source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c78867-5065-4c3b-9dde-1298285cf6b9",
   "metadata": {},
   "source": [
    "##### <srong><u> NOTE: Possible reasons for error in this step</u></strong>\n",
    "    1. If Open AI, model access/api key issue\n",
    "    2. For 3.5 gpt model, reduce \"top_k_ranking = 5\" in exp_config. This might effect the answer quality.\n",
    "    3. \"top_k_ranking\" is the number of chunks sent to llm and the token limitations might cause it to error out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d7ac4a-ae9c-4375-943e-632b1f022717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not include the specific revenue figure for 3M in 2022. To answer this question, I would need additional information or a context that contains the 2022 revenue for 3M.\n",
      "Total Input Token: 6534\n",
      "Total Output Token: 49\n"
     ]
    }
   ],
   "source": [
    "prediction = get_llm_response(model_id = exp_config['generation_llm_model'],prompt=ans_gen_prompt, invoke_azure_openai= invoke_azure_openai)\n",
    "print(prediction['resp'])\n",
    "print(f\"Total Input Token: {prediction['input']}\")\n",
    "print(f\"Total Output Token: {prediction['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6edda9-1b18-47bc-99a3-a68941e11140",
   "metadata": {},
   "source": [
    "### Bulk Processing - Proceed ONLY After Indexing is completed for all the files in notebook 01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab01d818-a060-458b-80f7-fb4a4b2f538a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\"What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\",\n",
    "             \"Is 3M a capital-intensive business based on FY2022 data?\",\n",
    "             \"Does Adobe have an improving operating margin profile as of FY2022? If operating margin is not a useful metric for a company like this, then state that and explain why.\",\n",
    "             \"Does Adobe have an improving Free cashflow conversion as of FY2022?\",\n",
    "             \"Answer the following question as if you are an equity research analyst and have lost internet connection so you do not have access to financial metric providers. According to the details clearly outlined within the P&L statement and the statement of cash flows, what is the FY2015 depreciation and amortization (D&A from cash flow statement) % margin for AMD?\",\n",
    "             \"From FY21 to FY22, excluding Embedded, in which AMD reporting segment did sales proportionally increase the most?\",\n",
    "             \"How much has the effective tax rate of American Express changed between FY2021 and FY2022?\",\n",
    "             \"What was the largest liability in American Express's Balance Sheet in 2022?\",\n",
    "             \"What is the year end FY2019 total amount of inventories for Best Buy? Answer in USD millions. Base your judgments on the information provided primarily in the balance sheet.\",\n",
    "             \"Are Best Buy's gross margins historically consistent (not fluctuating more than roughly 2% each year)? If gross margins are not a relevant metric for a company like this, then please state that and explain why.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0acc138d-9a82-455f-9cc4-efd8a820ce0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "Using exact matching in semantic search with ['2018'], ['3M']\n",
      "Using exact matching in text search with ['2018'], ['3M']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.002820253372192383\n",
      "****Total Input Token : 12622\n",
      "****Total Output Token : 250\n",
      "The capital expenditure amount for 3M in FY2018 was $1,577 million. This is reported as \"Purchases of property, plant and equipment (PP&E)\" in the \"Cash Flows from Investing Activities\" section of the cash flow statement.\n",
      "****Total Time taken for Metadata extraction - time : 1.50\n",
      "****Total Time taken for Metadata extraction - keywords : 0.72\n",
      "****Total Time taken for Metadata extraction - combined : 2.51\n",
      "****Total Time taken for Answer Generation : 4.73\n",
      "Is 3M a capital-intensive business based on FY2022 data?\n",
      "Using exact matching in semantic search with ['2022'], ['3M']\n",
      "Using exact matching in text search with ['2022'], ['3M']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.005777835845947266\n",
      "****Total Input Token : 12937\n",
      "****Total Output Token : 379\n",
      "Yes, 3M can be considered a capital-intensive business based on FY2022 data. The company's capital spending for property, plant, and equipment in 2022 was $1,749 million, and its net property, plant, and equipment as of December 31, 2022, was $9,178 million. These figures represent significant investments in long-term assets compared to the company's total assets and revenues.\n",
      "\n",
      "[Source File: 3M_2022_10K]\n",
      "[source page: 34, 38, 39]\n",
      "****Total Time taken for Metadata extraction - time : 1.85\n",
      "****Total Time taken for Metadata extraction - keywords : 1.06\n",
      "****Total Time taken for Metadata extraction - combined : 3.21\n",
      "****Total Time taken for Answer Generation : 9.25\n",
      "Does Adobe have an improving operating margin profile as of FY2022? If operating margin is not a useful metric for a company like this, then state that and explain why.\n",
      "Using exact matching in semantic search with ['2022'], ['Adobe']\n",
      "Using exact matching in text search with ['2022'], ['Adobe']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0018084049224853516\n",
      "****Total Input Token : 8393\n",
      "****Total Output Token : 372\n",
      "Yes, Adobe has an improving operating margin profile as of FY2022. The operating margin increased from 27% in fiscal 2021 to 28% in fiscal 2022, as evidenced by the provided context. This improvement is a positive indicator of the company's financial health and profitability. Operating margin is a useful metric for a company like Adobe, as it measures the proportion of revenue that remains after subtracting operating expenses, providing insight into the company's ability to control costs and generate profits from its core business operations.\n",
      "\n",
      "Source: ADOBE_2022_10K, page 43.\n",
      "****Total Time taken for Metadata extraction - time : 1.70\n",
      "****Total Time taken for Metadata extraction - keywords : 0.61\n",
      "****Total Time taken for Metadata extraction - combined : 3.27\n",
      "****Total Time taken for Answer Generation : 4.19\n",
      "Does Adobe have an improving Free cashflow conversion as of FY2022?\n",
      "Using exact matching in semantic search with ['2022'], ['Adobe']\n",
      "Using exact matching in text search with ['2022'], ['Adobe']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0025072097778320312\n",
      "****Total Input Token : 8648\n",
      "****Total Output Token : 303\n",
      "Based on the provided context, the information about Adobe's Free Cash Flow (FCF) conversion rate for the financial year 2022 is not available. The context discusses various financial aspects of Adobe but does not mention the FCF conversion rate for 2022. Therefore, it is not possible to determine if Adobe has shown an improvement in its FCF conversion rate as of the financial year 2022.\n",
      "****Total Time taken for Metadata extraction - time : 1.70\n",
      "****Total Time taken for Metadata extraction - keywords : 0.74\n",
      "****Total Time taken for Metadata extraction - combined : 2.78\n",
      "****Total Time taken for Answer Generation : 2.86\n",
      "Answer the following question as if you are an equity research analyst and have lost internet connection so you do not have access to financial metric providers. According to the details clearly outlined within the P&L statement and the statement of cash flows, what is the FY2015 depreciation and amortization (D&A from cash flow statement) % margin for AMD?\n",
      "Using exact matching in semantic search with ['2015'], ['AMD']\n",
      "Using exact matching in text search with ['2015'], ['AMD']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.004126310348510742\n",
      "****Total Input Token : 17737\n",
      "****Total Output Token : 406\n",
      "The Depreciation and Amortization (D&A) for the fiscal year 2015 was $94 million as per the provided context. To calculate the D&A percentage margin, we need to divide D&A by net revenue and multiply by 100. The net revenue for 2015 was $3,991 million.\n",
      "\n",
      "So, the calculation would be: ($94/$3,991)*100 = 2.36%\n",
      "\n",
      "Therefore, the Depreciation and Amortization (D&A) percentage margin for AMD in 2015 was approximately 2.36%.\n",
      "****Total Time taken for Metadata extraction - time : 2.24\n",
      "****Total Time taken for Metadata extraction - keywords : 0.81\n",
      "****Total Time taken for Metadata extraction - combined : 3.44\n",
      "****Total Time taken for Answer Generation : 9.74\n",
      "From FY21 to FY22, excluding Embedded, in which AMD reporting segment did sales proportionally increase the most?\n",
      "Using exact matching in semantic search with ['2021', '2022'], ['AMD']\n",
      "Using exact matching in text search with ['2021', '2022'], ['AMD']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0028510093688964844\n",
      "****Total Input Token : 10634\n",
      "****Total Output Token : 284\n",
      "The Gaming segment had the highest proportional increase in sales from FY21 to FY22, with a 21% increase.\n",
      "****Total Time taken for Metadata extraction - time : 3.03\n",
      "****Total Time taken for Metadata extraction - keywords : 0.55\n",
      "****Total Time taken for Metadata extraction - combined : 3.30\n",
      "****Total Time taken for Answer Generation : 2.60\n",
      "How much has the effective tax rate of American Express changed between FY2021 and FY2022?\n",
      "Using exact matching in semantic search with ['2021', '2022'], ['American Express', 'AmEx']\n",
      "Using exact matching in text search with ['2021', '2022'], ['American Express', 'AmEx']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0030105113983154297\n",
      "****Total Input Token : 10952\n",
      "****Total Output Token : 334\n",
      "The effective tax rate for American Express decreased from 24.6% in 2021 to 21.6% in 2022, a reduction of 3 percentage points.\n",
      "\n",
      "Source: [AMERICANEXPRESS_2022_10K, source page: 49]\n",
      "\n",
      "Source files: AMERICANEXPRESS_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 1.87\n",
      "****Total Time taken for Metadata extraction - keywords : 0.79\n",
      "****Total Time taken for Metadata extraction - combined : 3.57\n",
      "****Total Time taken for Answer Generation : 6.16\n",
      "What was the largest liability in American Express's Balance Sheet in 2022?\n",
      "Using exact matching in semantic search with ['2022'], ['American Express']\n",
      "Using exact matching in text search with ['2022'], ['American Express']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.009348392486572266\n",
      "****Total Input Token : 14155\n",
      "****Total Output Token : 300\n",
      "Based on the provided context, the exact values for American Express's liabilities in 2022 are not specified. The context discusses various aspects of American Express's financials, but it does not provide a comprehensive list of liabilities for 2022. To answer this question, the Consolidated Balance Sheet for 2022 is needed, which should contain a detailed breakdown of both assets and liabilities.\n",
      "****Total Time taken for Metadata extraction - time : 1.88\n",
      "****Total Time taken for Metadata extraction - keywords : 0.91\n",
      "****Total Time taken for Metadata extraction - combined : 2.56\n",
      "****Total Time taken for Answer Generation : 6.46\n",
      "What is the year end FY2019 total amount of inventories for Best Buy? Answer in USD millions. Base your judgments on the information provided primarily in the balance sheet.\n",
      "Using exact matching in semantic search with ['2019'], ['Best Buy']\n",
      "Using exact matching in text search with ['2019'], ['Best Buy']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.009576082229614258\n",
      "****Total Input Token : 12348\n",
      "****Total Output Token : 361\n",
      "The total amount of inventories for Best Buy at the end of the fiscal year 2019 was $5,409 million.\n",
      "\n",
      "Reference(s):\n",
      "[source page: 52] Consolidated Balance Sheets\n",
      "$ in millions, except per share and share amounts\n",
      "\n",
      "the following is a CSV table:\n",
      "{ ,0,1,2\n",
      "0,,\"February 2, 2019\",\"February 3, 2018\"\n",
      "...\n",
      "6,Merchandise inventories,\"5,409\",\"5,209\"\n",
      "...\n",
      "}\n",
      "\n",
      "[Source File: BESTBUY_2019_10K]\n",
      "****Total Time taken for Metadata extraction - time : 2.26\n",
      "****Total Time taken for Metadata extraction - keywords : 0.53\n",
      "****Total Time taken for Metadata extraction - combined : 2.39\n",
      "****Total Time taken for Answer Generation : 10.18\n",
      "Are Best Buy's gross margins historically consistent (not fluctuating more than roughly 2% each year)? If gross margins are not a relevant metric for a company like this, then please state that and explain why.\n",
      "Using exact matching in semantic search with ['2014', '2023', '...', '2013', '2012'], ['Best Buy']\n",
      "Using exact matching in text search with ['2023', '2014', '2013', '2012'], ['Best Buy']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.005026817321777344\n",
      "****Total Input Token : 13333\n",
      "****Total Output Token : 454\n",
      "Best Buy's gross margins have not been historically consistent, with fluctuations exceeding 2% in some years. For instance, the gross profit as a percentage of revenue for the Domestic segment was 23.9% in 2017, 23.4% in 2018, and 23.3% in 2019. This shows a fluctuation of more than 2% when comparing 2017 and 2019.\n",
      "\n",
      "[Source File: BESTBUY_2019_10K]\n",
      "[source page: 26, 33]\n",
      "****Total Time taken for Metadata extraction - time : 4.27\n",
      "****Total Time taken for Metadata extraction - keywords : 0.95\n",
      "****Total Time taken for Metadata extraction - combined : 2.96\n",
      "****Total Time taken for Answer Generation : 8.75\n"
     ]
    }
   ],
   "source": [
    "llm_answers =[]\n",
    "llm_contexts =[]\n",
    "latency_meta_time = []\n",
    "latency_meta_kwd = []\n",
    "latency_meta_comb = []\n",
    "latency_meta_ans_gen = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "for question in questions:   \n",
    "    print(question)\n",
    "    t0=time.time()\n",
    "    response = time_keyword_extraction(prompt=PROMPT_METADATA_GENERATION_time, question=question)\n",
    "    t1=time.time()\n",
    "    input_token1 = response['input']\n",
    "    output_token1 = response['output']\n",
    "    time_keyword_type, time_kwds, explanation = llm_ouput_to_json_time(response['resp'])\n",
    "    t2=time.time()\n",
    "    response = technical_keyword_extraction(prompt=PROMPT_METADATA_TECHNICAL_KWD, question=question)\n",
    "    t3=time.time()\n",
    "    input_token2 = response['input']\n",
    "    output_token2 = response['output']\n",
    "    kwds = llm_output_kwd(response['resp'])\n",
    "    t4=time.time()\n",
    "    company_keywords, time_kwds, kwds, rephrased_q, response = metadata_extraction(\n",
    "        prompt=PROMPT_METADATA_AND_QUERY_ReWr,\n",
    "        question =question,\n",
    "        time_kwds=time_kwds)\n",
    "    t5=time.time()\n",
    "    input_token3 = response['input']\n",
    "    output_token3 = response['output']\n",
    "    time_key_in_years, q_kwds = get_list_variables(\n",
    "        company_keywords,\n",
    "        time_kwds,\n",
    "        kwds)\n",
    "    contexts = get_context( rephrased_q,\n",
    "                           q_kwds,\n",
    "                           time_keyword_type,\n",
    "                           time_key_in_years,\n",
    "                           kwds,\n",
    "                           time_kwds,\n",
    "                           company_keywords)\n",
    "    ranked_contexts, top_k_contexts = rerank_context(rephrased_q,\n",
    "                                                     q_kwds,time_kwds,\n",
    "                                                     company_keywords,\n",
    "                                                     contexts )\n",
    "    ans_gen_prompt = PROMPT_ANS_GENERATION.format(\n",
    "        QUERY=question,\n",
    "        CONTEXT=top_k_contexts,\n",
    "        TIME_KWDS=time_kwds,\n",
    "        REPHARSED_QUERY=rephrased_q,\n",
    "        most_recent_quarter=\"Q1'24\")\n",
    "    t6=time.time()\n",
    "    prediction = get_llm_response(model_id = exp_config['generation_llm_model'],prompt=ans_gen_prompt, invoke_azure_openai= invoke_azure_openai)\n",
    "    t7=time.time()\n",
    "    input_token4 = prediction['input']\n",
    "    output_token4 = prediction['output']\n",
    "    total_input_token = input_token1 + input_token2 + input_token3 + input_token4\n",
    "    total_output_token = output_token1 + output_token2 + output_token3 + output_token4\n",
    "    input_tokens = input_tokens + [total_input_token]\n",
    "    output_tokens = output_tokens + [total_output_token]\n",
    "    print(f\"****Total Input Token : {total_input_token}\")\n",
    "    print(f\"****Total Output Token : {total_output_token}\")\n",
    "    answer = prediction['resp']\n",
    "    llm_answers = llm_answers + [answer.replace(\"\\n\", \"\")]\n",
    "    llm_contexts = llm_contexts +[ranked_contexts]\n",
    "    latency_meta_time = latency_meta_time + [t1-t0]\n",
    "    latency_meta_kwd = latency_meta_kwd + [t3-t2]\n",
    "    latency_meta_comb = latency_meta_comb + [t5-t4]\n",
    "    latency_meta_ans_gen = latency_meta_ans_gen + [t7-t6]\n",
    "    print(answer)\n",
    "    print(f\"****Total Time taken for Metadata extraction - time : {(t1-t0):.2f}\")\n",
    "    print(f\"****Total Time taken for Metadata extraction - keywords : {(t3-t2):.2f}\")\n",
    "    print(f\"****Total Time taken for Metadata extraction - combined : {(t5-t4):.2f}\")\n",
    "    print(f\"****Total Time taken for Answer Generation : {(t7-t6):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b84da-db81-4e69-b8cf-e275c3f5cb79",
   "metadata": {},
   "source": [
    "### Write to CSV for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cf8b207-b6c5-4389-83b0-ee63062a4794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"gpt\" in exp_config['generation_llm_model']:\n",
    "    output_file = '../outputs/rag_outputs/output_openai.csv'\n",
    "else:\n",
    "    output_file = '../outputs/rag_outputs/output_mistral.csv'\n",
    "# Open the input CSV file for reading\n",
    "with open('../data/selected_samples.csv', 'r') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    # Open the output CSV file for writing\n",
    "    fieldnames = reader.fieldnames  # Get the fieldnames from the input file\n",
    "    \n",
    "    # Add the new column names to the fieldnames list\n",
    "    new_columns = ['latency_meta_time', 'latency_meta_kwd', 'latency_meta_comb', 'latency_meta_ans_gen', 'input_tokens', 'output_tokens']\n",
    "    fieldnames.extend(new_columns)\n",
    "    \n",
    "    # # Open the output CSV file for writing\n",
    "    # fieldnames = reader.fieldnames  # Get the fieldnames from the input file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        # Lists of new values to be updated in the specified columns\n",
    "        llm_ans = llm_answers\n",
    "        llm_contex = llm_contexts\n",
    "        \n",
    "        # Iterate over the rows in the input file and the new values\n",
    "        for row_index, row in enumerate(reader, start=1):\n",
    "            # Update the values in the desired columns for the current row\n",
    "            if row_index <= len(llm_ans):\n",
    "                row['llm_answer'] = llm_ans[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['llm_contexts'] = llm_contex[row_index - 1]\n",
    "                \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_time'] = latency_meta_time[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_kwd'] = latency_meta_kwd[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_comb'] = latency_meta_comb[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_ans_gen'] = latency_meta_ans_gen[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['input_tokens'] = input_tokens[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['output_tokens'] = output_tokens[row_index - 1]\n",
    "            \n",
    "            \n",
    "            # Write the updated row to the output file\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91114f51-62c9-4fc0-b72c-4036596661e3",
   "metadata": {},
   "source": [
    "### <center>----------EOF---------</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
