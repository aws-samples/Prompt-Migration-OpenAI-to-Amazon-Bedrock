{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa418c60-1d6d-44cc-a083-90463c123288",
   "metadata": {},
   "source": [
    "# Investment Analyst Assistant Indexing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a2a8d",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "Copyright 2024 Amazon Web Services, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a2a8d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook allows you to upload and preprocess documents, create chunks, generate embeddings, and upload to OpenSearch Serverless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba22526-e7d2-4d64-a317-439807e33e5d",
   "metadata": {},
   "source": [
    "### STEP 0:  Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f134e",
   "metadata": {},
   "source": [
    "NOTE: Warnings and in some case, version errors can be ignored for package installation. Those are due to version updates. Only change versions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ac85cc-c176-4094-a20f-1a11a621b92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install missing packages or modules\n",
    "!pip install boto3 --quiet\n",
    "!pip install opensearch-py==2.4.2 --quiet\n",
    "!pip install retry --quiet\n",
    "!pip install amazon-textract-textractor==1.3.5 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94288e",
   "metadata": {},
   "source": [
    "#### Adding Project Directory to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05640244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust this path as needed\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08ec5f-30c8-4bba-b49b-fc0efff0665d",
   "metadata": {},
   "source": [
    "### STEP 1:  Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70007510-a739-442c-a800-8c59dc3a483c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from opensearchpy.helpers import bulk\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "from io import StringIO\n",
    "import csv\n",
    "from retry import retry\n",
    "import urllib3\n",
    "import logging\n",
    "import sagemaker\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b894f140-d953-4211-8e98-9f898de59e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your AWS account number is: 123123123123\n"
     ]
    }
   ],
   "source": [
    "session = boto3.Session()\n",
    "sts_client = session.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "print(f\"Your AWS account number is: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6d37-008c-4807-8228-70c8a97c2ab2",
   "metadata": {},
   "source": [
    "#### Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e04456-a2e8-4376-b1e6-dd443496df4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Amzon S3 bucket names need to be unique and should be per Amazon S3 naming rules.\n",
    "## Please refet to this for naming rules  https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html\n",
    "## Bucket names are checked in the next step and the naming conventions might cause errors.\n",
    "exp_config = {\n",
    "    'region': 'us-west-2',\n",
    "    'source_bucket': f'source-files-{account_id}', #name of the S3 Bucket to upload the PDF documents\n",
    "    'service_bucket': f'service-bucket-{account_id}', #Different bucket than source to store metadata file \n",
    "    'embb_model': 'amazon.titan-embed-text-v1'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3827e",
   "metadata": {},
   "source": [
    "#### Create Amazon s3 bucked and upload the sample docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd88e240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket source-files-123123123123 already exists\n",
      "Bucket service-bucket-123123123123 already exists\n",
      "Uploading sample files to service-bucket-123123123123 bucket\n",
      "Uploading Complete!\n"
     ]
    }
   ],
   "source": [
    "# Create S3 client\n",
    "session = boto3.Session(region_name=exp_config['region'])\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "# Bucket name\n",
    "source_bucket_name = exp_config['source_bucket']\n",
    "service_buck_name = exp_config['service_bucket']\n",
    "# Check if source bucket exists\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=source_bucket_name)\n",
    "except s3_client.exceptions.ClientError as e:\n",
    "    # Get exception code\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == '404':\n",
    "        print(f\"Bucket {source_bucket_name} does not exist, creating source bucket...\")\n",
    "        # Create bucket\n",
    "        s3_client.create_bucket(Bucket=source_bucket_name, CreateBucketConfiguration={'LocationConstraint': exp_config['region']})\n",
    "    else:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"Bucket {source_bucket_name} already exists\")\n",
    "\n",
    "# Check if service bucket exists\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=service_buck_name)\n",
    "except s3_client.exceptions.ClientError as e:\n",
    "    # Get exception code\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == '404':\n",
    "        print(f\"Bucket {service_buck_name} does not exist, creating source bucket...\")\n",
    "        # Create bucket\n",
    "        s3_client.create_bucket(Bucket=service_buck_name, CreateBucketConfiguration={'LocationConstraint': exp_config['region']})\n",
    "    else:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"Bucket {service_buck_name} already exists\")\n",
    "\n",
    "# Upload files from the folder to the root of the bucket\n",
    "print(f\"Uploading sample files to {service_buck_name} bucket\")\n",
    "folder_path = '../data/sample_docs/'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        object_key = os.path.relpath(file_path, folder_path)\n",
    "        s3_client.upload_file(file_path, source_bucket_name, object_key)\n",
    "\n",
    "print(\"Uploading Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04a7ad-7119-4b1f-83c5-b7cdb08b1032",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 2:  Create Openserach collection/host if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60939fa7-cba3-44b4-9220-16955766e158",
   "metadata": {},
   "source": [
    "##### This step will Create Opensearch collection for Indexing the document. It will Only create the collection if it doesn't exist. If Exists, It will simply output the host whaich can be copied to configs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b506027-2ee3-4957-a250-9323fe968e68",
   "metadata": {},
   "source": [
    "#### Get Sagemaker Role ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771b1769-5ce1-40d4-9c76-8714038ee786",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::123123123123:role/service-role/AmazonSageMaker-ExecutionRole-20240621T171347\n"
     ]
    }
   ],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "notebook_role_arn = sagemaker.get_execution_role()\n",
    "print (notebook_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5547a5",
   "metadata": {},
   "source": [
    "#### (OPTIONAL) These permissions are Already provided. In Case of some access issues, update the role as follows\n",
    "Goto IAM>Roles\n",
    "* Look for the IAM Role(output from previous cell execution) that the studio is assuming. \n",
    "* Create an inline policy with the policy definition below and attach it to the IAM role.\n",
    "* Also attach \"arn:aws:iam::aws:policy/AmazonTextractFullAccess\" to the iam role. Add the other policies mentioned below only if necessary. most of it should already be configured. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4ed0a-0865-43a0-8691-95832929c94e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Make sure the role has following permissions\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"aoss:*\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:InvokeModel\",\n",
    "                \"bedrock:InvokeModelWithResponseStream\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44cc24-976a-41ea-9d31-7c60b037271a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Additional Managed policy required\n",
    "##### arn:aws:iam::aws:policy/AmazonOpenSearchServiceFullAccess\n",
    "##### arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n",
    "##### arn:aws:iam::aws:policy/AmazonTextractFullAccess\n",
    "\n",
    "##### <strong>Please note that these roles are permissible and are meant only for testing and shall NOT be used for production.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2058d-ab49-43f8-b5e3-f7bd6d65ba70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initiate OpenSearch Serverless Instance\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9780fd03-433f-471b-9e89-c733582556e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, exp_config['region'], \"aoss\")\n",
    "opensearch_client = boto3.client('opensearchserverless')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e191654-d89e-4c4f-9175-fbc6c2f4ac8d",
   "metadata": {},
   "source": [
    "##### Input name variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e85825ac-51e2-4b51-9f81-781f3a95948e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_name = 'my-expt-collection' # Name for Opensearch Serverless Collection, change as necessary\n",
    "index_name = \"expt_index\" # Name for Index, change as necessary\n",
    "data_access_policy_name = \"data-access-expt-policy\" # Name for data access policy, change as necessary\n",
    "encryption_policy_name = \"expt-encryption-policy\" # Name for encryption policy, change as necessary\n",
    "network_policy_name = \"expt-network-policy\" # Name for network policy, change as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdf986-d826-4516-8b3d-b0a3cc420c7b",
   "metadata": {},
   "source": [
    "##### Opensearch Access Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03bb38e6-5381-48f2-bf8f-6d56eea8d46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Access Policy\n",
    "data_access_policy = json.dumps(\n",
    "[\n",
    "  {\n",
    "    \"Rules\": [\n",
    "      {\n",
    "        \"Resource\": [\n",
    "          \"collection/*\"\n",
    "        ],\n",
    "        \"Permission\": [\n",
    "          \"aoss:CreateCollectionItems\",\n",
    "          \"aoss:DeleteCollectionItems\",\n",
    "          \"aoss:UpdateCollectionItems\",\n",
    "          \"aoss:DescribeCollectionItems\"\n",
    "        ],\n",
    "        \"ResourceType\": \"collection\"\n",
    "      },\n",
    "      {\n",
    "        \"Resource\": [\n",
    "          \"index/*/*\"\n",
    "        ],\n",
    "        \"Permission\": [\n",
    "          \"aoss:CreateIndex\",\n",
    "          \"aoss:DeleteIndex\",\n",
    "          \"aoss:UpdateIndex\",\n",
    "          \"aoss:DescribeIndex\",\n",
    "          \"aoss:ReadDocument\",\n",
    "          \"aoss:WriteDocument\"\n",
    "        ],\n",
    "        \"ResourceType\": \"index\"\n",
    "      }\n",
    "    ],\n",
    "    \"Principal\": [\n",
    "        notebook_role_arn\n",
    "    ],\n",
    "    \"Description\": \"data-access-rule\"\n",
    "  }\n",
    "]\n",
    ")\n",
    "# Convert policy to JSON string\n",
    "data_access_policy_json = json.dumps(data_access_policy)\n",
    "\n",
    "#Encryption Policy\n",
    "\n",
    "encryption_policy = {\n",
    "    \"Rules\": [\n",
    "        {\n",
    "            \"Resource\": [\n",
    "                f\"collection/{collection_name}\"\n",
    "            ],\n",
    "            \"ResourceType\": \"collection\"\n",
    "        }\n",
    "    ],\n",
    "    \"AWSOwnedKey\": True\n",
    "}\n",
    "\n",
    "#Network Policy\n",
    "network_policy = json.dumps(\n",
    "[\n",
    "  {\n",
    "    \"Rules\": [\n",
    "      {\n",
    "        \"Resource\": [\n",
    "          f\"collection/{collection_name}\"\n",
    "        ],\n",
    "        \"ResourceType\": \"dashboard\"\n",
    "      },\n",
    "      {\n",
    "        \"Resource\": [\n",
    "          f\"collection/{collection_name}\"\n",
    "        ],\n",
    "        \"ResourceType\": \"collection\"\n",
    "      }\n",
    "    ],\n",
    "    \"AllowFromPublic\": True\n",
    "  }\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bf75b-76ff-4ed8-bb7b-8df55f556324",
   "metadata": {},
   "source": [
    "#### Create Open Search Serverless Collection\n",
    "\n",
    "##### NOTE: This step might take couple minutes to complete. This is because OpenSearch Serverless is being provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7a4eea-66e0-4c60-a36d-b7cac59dc316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'my-expt-collection' already exists.\n",
      "OpenSearch endpoint for 'my-expt-collection' collection: collectionendpoint.us-west-2.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "collection_endpoint = None\n",
    "try:\n",
    "    # Check if the collection already exists\n",
    "    response = opensearch_client.list_collections()\n",
    "    existing_collections = [collection['name'] for collection in response['collectionSummaries']]\n",
    "    \n",
    "    if collection_name in existing_collections:\n",
    "        print(f\"Collection '{collection_name}' already exists.\")\n",
    "    else:\n",
    "        response = opensearch_client.create_security_policy(\n",
    "            name=network_policy_name,\n",
    "            policy=network_policy,\n",
    "            type=\"network\")\n",
    "        response = opensearch_client.create_access_policy(\n",
    "            name= data_access_policy_name,\n",
    "            type='data',\n",
    "            policy=data_access_policy)\n",
    "        response = opensearch_client.create_security_policy(\n",
    "            name= encryption_policy_name,\n",
    "            policy=json.dumps(encryption_policy),\n",
    "            type='encryption')\n",
    "\n",
    "        # Create the collection\n",
    "        response = opensearch_client.create_collection(\n",
    "            name=collection_name,\n",
    "            type='VECTORSEARCH')\n",
    "        print(f\"Collection '{collection_name}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "response = opensearch_client.batch_get_collection(\n",
    "    names=[collection_name])\n",
    "while collection_endpoint == None:\n",
    "    try:\n",
    "        response = opensearch_client.batch_get_collection(\n",
    "            names=[collection_name])\n",
    "        collection_detail = response['collectionDetails'][0]\n",
    "        collection_endpoint = collection_detail['collectionEndpoint'].replace(\"https://\", \"\")\n",
    "        break\n",
    "    except:\n",
    "        time.sleep(30)\n",
    "print(f\"OpenSearch endpoint for '{collection_name}' collection: {collection_endpoint}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6300f6-9c2a-4339-9d67-5b8d076cb5b3",
   "metadata": {},
   "source": [
    "##### Updating Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "836ca7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../libraries/iaa/configs.py', 'r') as config_file:\n",
    "    lines = config_file.readlines()\n",
    "with open('../libraries/iaa/configs.py', 'w') as config_file:\n",
    "    for line in lines:\n",
    "        if line.startswith(\"OPEN_SEARCH_HOST\"):\n",
    "            config_file.write(f\"OPEN_SEARCH_HOST = '{collection_endpoint}'\\n\")\n",
    "        else:\n",
    "            config_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8bbb2",
   "metadata": {},
   "source": [
    "#### Importing Project relevant Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f16e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from libraries.iaa.textSplitter import TextSplitter\n",
    "from libraries.iaa.textractPdfParser import TextractPdfParser\n",
    "from libraries.iaa.text_summary import text_summary\n",
    "from libraries.iaa.sectionsplitter import sectionSplitter\n",
    "from libraries.iaa.embedUpload import bulkLoadChunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740da6b-7941-4ac5-998e-67ebeb36aa81",
   "metadata": {},
   "source": [
    "### STEP 3:  Create index in collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e262b-0142-4f48-90a1-dc4bda807f70",
   "metadata": {},
   "source": [
    "##### Note: Only need to run for a index (index name) once. Run this if you want to create new index within same collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9df4ee6c-041c-4d42-b4cb-3431d751ffce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'expt_index' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the index mapping\n",
    "index_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector_field\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"doc_name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"doc_link\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "client = OpenSearch(\n",
    "        hosts=[{'host': collection_endpoint, 'port': 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "# Create the index with the specified mapping\n",
    "if not client.indices.exists(index=index_name):\n",
    "        # Create the index if it doesn't exist\n",
    "        client.indices.create(index=index_name, body=index_mapping)\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47df65-f39e-4fa9-a08a-35b358a15c82",
   "metadata": {},
   "source": [
    "### STEP 4:  Getting List of files from S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d81c33a-796e-425a-a203-d0b22dae6029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Get a list of object keys (file names) in the bucket\n",
    "objects = s3.list_objects_v2(Bucket=exp_config['source_bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a7de1-4cf6-415b-b266-e0593f7cf774",
   "metadata": {},
   "source": [
    "### STEP 5:  Chunking documents and uploading to Opensearch Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50aa4d-de1b-4dcb-8ba5-9de7b97a4f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### This step parses the documents, chunks it and then indexes it. It will take around 30 minutes for all the sample documents. However, you may proceed to notebook : \"02_Query-rewrite_Retrieval_and_Generation_Notebook_OpenAI\" after ypu see the first \"Uploading documents in OpenSearch Completed\" in the output. This means that the first document has been processed. The processing will happen in the background as we work through the other notebooks.\n",
    "##### <strong> DO NOT CLOSE THE NOTEBOOK UNTIL THE PROCESS IS COMPLETE FOR ALL THE FILES </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c431cb0-e1b9-4015-81a9-1e152802facd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Job still in progress...\n",
      "Job SUCCEEDED...\n",
      "Retrieving content\n",
      "Embedding vector for section chunks started\n",
      "Embedding vector for section chunks completed\n",
      "Adding chunks to index\n",
      "Adding chunks to index complete\n",
      "Uploading documents in OpenSearch Completed\n",
      "Indexing Completed!\n"
     ]
    }
   ],
   "source": [
    "# Print the file names\n",
    "for obj in objects.get('Contents', []):\n",
    "    file= str(obj['Key'])\n",
    "    sec = sectionSplitter(sourcebucket = exp_config['source_bucket'], doc_path = file, indexId = index_name, embeddingsModel = exp_config['embb_model'], serviceBucket = exp_config['service_bucket'])\n",
    "    chunks = sec.doc2index()\n",
    "    bulkLoadChunks(chunks, indexId= index_name, embeddingsModel = exp_config['embb_model'])\n",
    "print(\"Indexing Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06efb5f-ae22-4ea5-bee5-9126c1411da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
