{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b779787-fb64-48da-be54-9318ed3ee5f3",
   "metadata": {},
   "source": [
    "# Investment Analyst Assistant Retreival Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e9909",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "Copyright 2024 Amazon Web Services, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342aafd-8be8-4d34-af97-a7723514392d",
   "metadata": {},
   "source": [
    "##### This notebook allows you to generate metadata forom the question. This metadata will be used to retreived specific chunks from the Opensearch Index which is then sent to the LLM to generate answer for the specific question. This notebook is configured to use \"anthropic.claude\" LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e6f6b",
   "metadata": {},
   "source": [
    "\n",
    "**Question & Answer Pipeline**: \n",
    "![iaa_arch.png](../images/generation.png)\n",
    "<!-- <center>\n",
    "<img src=\"../src/generation.png\" alt=\"Investment Analyst Assistant Architecture\" width=\"400\"/>\n",
    "</center> -->\n",
    "\n",
    "1. A user provides a query.\n",
    "2. The Query Expansion/Enrichment module uses prompt engineering techniques to expand and enrich the query.\n",
    "3. The expanded query embeddings are generated using the Titan Text Embedding Model.\n",
    "4. The Retrieval component retrieves relevant chunks from the OpenSearch instance based on the query embeddings.\n",
    "5. The retrieved chunks are passed to the Answer Generation module, which involves prompt engineering and interaction with a language model (OpenAI or Claude) to generate the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265c919-8e10-487e-98b5-e28a049e1533",
   "metadata": {},
   "source": [
    "### STEP 0:  Reset And Install missing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a25d6e",
   "metadata": {},
   "source": [
    "NOTE: Warnings and in some case, version errors can be ignored for package installation. Those are due to version updates. Only change versions if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a79159b-b97b-4679-8a45-e874e51fba26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4997921f-b4e9-4e38-a66a-76576722e6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install requests_toolbelt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf572cda",
   "metadata": {},
   "source": [
    "#### Adding Project Directory to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89954e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust this path as needed\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab5e84-fdb5-4942-b769-546b5133da06",
   "metadata": {},
   "source": [
    "### STEP 1: Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45604a88-6167-426e-8bfe-de692af0214f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from libraries.iaa.experiments.utils_exp import get_titan_text_embedding, results_fusion\n",
    "from libraries.iaa.query_transformation.ExtractQueryMetadata import ExtractQueryMetadata\n",
    "from libraries.iaa.reranker.SearchRanker import SearchRanker\n",
    "from libraries.iaa.retrieval.OpenSearchRetrieval import OpenSearchRetrieval\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from libraries.iaa.query_transformation.QueryMetaExtractor import QueryMetaExtractor\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import csv\n",
    "import libraries.iaa.configs as configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d8202-a25b-4b90-82d7-89c8729ac9ab",
   "metadata": {},
   "source": [
    "### STEP 2: Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5ae1a5-dc58-4e37-b5c7-a1e7f5050967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_config = {\n",
    "    'type_rets': ['fusion'], #fusion (text and embb) is selected for this workshop because it generated the best results\n",
    "    'opensearch_host': configs.OPEN_SEARCH_HOST, # copy the url created in the index creation notebook\n",
    "    'generation_llm_model': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    'index_name_embb': 'expt_index', #name of the index created in the index creation notebook\n",
    "    'index_name_text': 'expt_index', #name of the index created in the index creation notebook\n",
    "    'top_k_ranking': 20,\n",
    "    'top_k_retrieval': 30,\n",
    "    'emb_name': 'vector_field',\n",
    "    'region': configs.REGION\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcd93a-a2c4-4ffe-834b-818908b07031",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Input Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e208e593-d4a8-42c2-a826-071810ee99e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What was the revenue for 3M in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d769c4-4fbd-4b87-8bfe-0fb3840eb148",
   "metadata": {},
   "source": [
    "### STEP 3 : Rephrase And Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89086c-bc41-4545-ad0f-18fa127fb9e7",
   "metadata": {},
   "source": [
    "##### Initialize Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97aa9d57-0a52-40f1-b1dc-45fdfb8bd134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=exp_config['region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d79a2-4046-488b-bbfe-1330e4c2bfe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prompt to Extract Metadata from the Quesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e0d30a-7759-4dbb-b773-dc5b0cadf367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Prompt To Extract Time Related Keyword\n",
    "\n",
    "PROMPT_METADATA_GENERATION_time = \"\"\"\n",
    "\\n\\nHuman:\n",
    "You a financial editer that looks at a user questions and rephrases it accurately for better search and retrieval tasks.\n",
    "\n",
    "Financial question related to yearly and Quarterly financial Reports: {query} \\n\n",
    "Current year is {most_recent_year}\n",
    "Current quarter is {most_recent_quarter}\n",
    "<task>\n",
    "Given a user question, identify the following metadata a list of time-related keywords based on instruction below\n",
    "1. time_keyword_type: identifies what type of time range user is requesting for - range of years, range of quarters, specific years or specific quarters, none\n",
    "2. time_keywords: these keywords expand the year or quarter period if time_keyword_type is \"range of periods\" else it will be formatted version of year in YYYY format or quarter in Q'YY format.\n",
    "</task>\n",
    "\n",
    "<instruction>\n",
    "1. Identify whether the user is asking for a date range or specific set of years or quarters. If there is no year or quarter mention leave time_keyword blank\n",
    "2. If the user is requesting for specific year or years return year in YYYY format.\n",
    "3. If the user is requesting for specific quarter or quarters return quarter in Q'YY format. Example Q2'24, Q1'23\n",
    "4. If the user is requesting for documents in a specific range of time between two period, fill the year or quarter information between the time ranges.\n",
    "5. If the user is requesting for last N years, count backward from current year 2024\n",
    "6. If the user is requesting for last N quarters, count backward from current quarter and year Q1 2024\n",
    "<instruction>\n",
    "\n",
    "<examples>\n",
    "what was Google's net profit?\n",
    "time_keyword_type: none\n",
    "time_keywords: none\n",
    "explanation: no quarter or year mentioned\n",
    "\n",
    "What was Amazon's total sales in 2022?\n",
    "time_keyword_type: specific_year\n",
    "time_keywords: 2022\n",
    "\n",
    "What was Apple's revenue in 2019 compared to 2018?\n",
    "time_keyword_type: specific_year\n",
    "time_keywords: 2018, 2019\n",
    "explanation: the user is requesting to compare 2 different years\n",
    "\n",
    "Which of Disney's business segments had the highest growth in sales in Q4 F2023?\n",
    "time_keyword_type: specific_quarter\n",
    "time_keywords: Q4 2023\n",
    "\n",
    "How did Netflix's quarterly spending on research change as a percentage of quarterly revenue change between Q2 2019 and Q4 2019?\n",
    "time_keyword_type: range_quarter\n",
    "time_keywords: Q2 2019, Q3 2019, Q4 2019\n",
    "explanation: the quarters between Q2 2019 and Q4 2019 are Q2 2019, Q3 2019 and Q4 2019\n",
    "\n",
    "What was Spotify's growth in the last 5 quarters?\n",
    "time_keyword_type: range_quarter\n",
    "time_keywords: Q4 2023, Q3 2023, Q2 2023, Q1 2023, Q4 2024\n",
    "explanation: Since current quarter is Q1 2024, the last 3 quarters are Q4 2023, Q3 2023, Q2 2023\n",
    "\n",
    "In their 10-K filings, has Norweigean Cruise mentioned any negative environmental or weather-related impacts to their business in the last four years?\n",
    "time_keyword_type: range_year\n",
    "time_keywords: 2020, 2021, 2022, 2023\n",
    "explanation: Since the current year is 2024, the last four years are 2020, 2021, 2022 and 2023.\n",
    "</examples>\n",
    "\n",
    "\n",
    "Return a JSON object with the following fields:\n",
    "   - 'time_keyword_type': a list of time-related keywords\n",
    "   - 'time_keywords': a list of technical keywords\n",
    "   - 'explanation': explanation of you chose a certain time_keyword type and time keyword\n",
    "\n",
    "\\n\\nAssistant:The metadata for the user question {query}:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23af28ce-f6ef-48d4-8c76-af8b1d1dd257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Prompt To Extract technical Keywords\n",
    "\n",
    "PROMPT_METADATA_TECHNICAL_KWD = \"\"\"\n",
    "\n",
    "Human: imagine you are a financial analyst looking to answer the question {query} in 10k/10q documents.\n",
    "\n",
    "What are some of the keywords you would use for searching the documents based on the question?\n",
    "<instruction>\n",
    "1. Do not include company names, document names and timelines\n",
    "2. Generate 5-6 important list of comma separated keywords within a single <keywords></keywords> tag.\n",
    "3. Focus more on what sections of the document you would look at and add that to the keyword\n",
    "4. Do not add keywords that are not part of the question\n",
    "</instruction>\n",
    "\n",
    "\n",
    "Assistant:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fd46bb-364e-499b-8af1-25d71de36fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Prompt To Extract Metadata\n",
    "PROMPT_METADATA_AND_QUERY_ReWr = \"\"\"\n",
    "\\n\\nHuman:\n",
    "You a financial editer that looks at a user questions and rephrases it accurately for better search and retrieval tasks.\n",
    "\n",
    "Financial question related to yearly and Quarterly financial Reports: {query} \\n\n",
    "<task>\n",
    "Given a user question, identify the following metadata\n",
    "   - 'technical_keywords': a list of relevant keywords from question\n",
    "   - 'company_keywords': a list of company names\n",
    "   - 'rephrased_question': the full rephrased question string\n",
    "</task>\n",
    "\n",
    "<time_keywords>\n",
    "{time_kwds}\n",
    "</time_keywords>\n",
    "\n",
    "<technical_keywords>\n",
    "1. Generate a comprehensive list of all possible keywordsthat are relevant based on sections you would typically find in a financial document.\n",
    "2. Include different alternatives to the keywords, be imaginative.\n",
    "3. Remove the company name and document name from keyword list.\n",
    "</technical_keywords>\n",
    "\n",
    "<company_keywords>\n",
    "Generate a list of company names that are mentioned in the question.\n",
    "</company_keywords>\n",
    "\n",
    "<rephrased_question>\n",
    "1. Generate the keywords and rephrase the question to make it very clear\n",
    "2. Expand any acronyms and abbreviations in the original question by providing the full term. Include both the original abbreviated version and the expanded version in the rephrased question.\n",
    "</rephrased_question>\n",
    "\n",
    "Return a JSON object with the following fields:\n",
    "   - 'technical_keywords': a list of relevant keywords from question\n",
    "   - 'company_keywords': a list of company names\n",
    "   - 'rephrased_question': the full rephrased question string\n",
    "\n",
    "\n",
    "\\n\\nAssistant:The metadata for the user question {query}:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43baae-687d-4224-8207-7a33beed40ca",
   "metadata": {},
   "source": [
    "#### Metadata Generation Using prompts and Claude LLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b591165-5c37-4bcf-97a6-d35957569a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Function to get the LLM response in desired format\n",
    "def llm_ouput_to_json_time(llm_output):\n",
    "    # Use regular expressions to find content between curly braces\n",
    "    pattern = r\"\\{([^}]*)\\}\"\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "    if len(matches) < 1:\n",
    "        return \"\", []\n",
    "    try:\n",
    "        json_obj = json.loads(\"{\" + matches[0] + \"}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {str(e)}\")\n",
    "        return \"\", []\n",
    "    return (json_obj[\"time_keyword_type\"], json_obj[\"time_keywords\"], json_obj[\"explanation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377aaf8-8095-4cb5-a69a-62ab74e619ad",
   "metadata": {},
   "source": [
    "#### Time Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19136fa3-cad2-4a12-b3e0-b8d3bd1c5464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time keywords generated: ('specific_year', ['2022'], \"The user is requesting the revenue for 3M in a specific year, which is 2022. Therefore, the time_keyword_type is 'specific_year' and the time_keyword is '2022'.\")\n",
      "Total Input Token: 906\n",
      "Total Output Token: 83\n"
     ]
    }
   ],
   "source": [
    "def time_keyword_extraction(prompt, question, bedrock_client): \n",
    "    prompt_format = prompt.format(\n",
    "        query=question, most_recent_quarter=\"Q1'24\", most_recent_year=2024)\n",
    "    body = json.dumps({\n",
    "        # \"prompt\": prompt_format,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\":[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt_format\n",
    "            }]\n",
    "        }],\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0,\n",
    "        \"stop_sequences\": [\"Human\"]\n",
    "    })\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId = exp_config['generation_llm_model'],\n",
    "        accept = 'application/json',\n",
    "        contentType = 'application/json',\n",
    "        body=body\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    llm_output = response_body.get('content')[0]['text']\n",
    "\n",
    "    time_keyword_type, time_kwds, explanation = llm_ouput_to_json_time(llm_output)\n",
    "    input_token = int(response_body.get('usage')['input_tokens'])\n",
    "    output_token = int(response_body.get('usage')['output_tokens'])\n",
    "    return time_keyword_type, time_kwds, explanation, input_token, output_token\n",
    "time_keyword_type, time_kwds, explanation, input_token, output_token = time_keyword_extraction(prompt=PROMPT_METADATA_GENERATION_time, question = question, bedrock_client=bedrock_client)\n",
    "print(f\"Time keywords generated: {time_keyword_type, time_kwds, explanation}\")\n",
    "\n",
    "print(f\"Total Input Token: {input_token}\")\n",
    "print(f\"Total Output Token: {output_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "619a7193-d396-4a26-8e63-89ab463c68ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Function to get the LLM response in desired format\n",
    "def llm_output_kwd(llm_output):\n",
    "    soup = BeautifulSoup(llm_output, \"html.parser\")\n",
    "    keywords = soup.find(\"keywords\".lower())\n",
    "\n",
    "    keywords = re.sub(\"<[^<]+>\", \"\", str(keywords))\n",
    "    keywords = keywords.strip()\n",
    "    if keywords:\n",
    "        keywords = keywords.split(\",\")\n",
    "        keywords = [keyword.strip() for keyword in keywords]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ec67e-af23-41d2-8090-9fbef68f49b1",
   "metadata": {},
   "source": [
    "#### Technical Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfa5b2df-33e6-437b-bbc6-f45c8d18dc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['revenue', 'sales', 'income', 'financial performance', 'financial results']\n",
      "Total Input Token: 148\n",
      "Total Output Token: 20\n"
     ]
    }
   ],
   "source": [
    "def technical_keyword_extraction(prompt, question, bedrock_client):\n",
    "    prompt_format = prompt.format(query=question)\n",
    "    body = json.dumps({\n",
    "        # \"prompt\": prompt_format,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\":[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt_format\n",
    "            }]\n",
    "        }],\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0,\n",
    "        \"stop_sequences\": [\"Human\"]\n",
    "    })\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId = exp_config['generation_llm_model'],\n",
    "        accept = 'application/json',\n",
    "        contentType = 'application/json',\n",
    "        body=body\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    llm_output = response_body.get('content')[0]['text'] #('content')[0]['text'] or ('completion')\n",
    "    input_token = int(response_body.get('usage')['input_tokens'])\n",
    "    output_token = int(response_body.get('usage')['output_tokens'])\n",
    "    kwds = llm_output_kwd(llm_output)\n",
    "    return kwds, input_token, output_token \n",
    "kwds, input_token, output_token  = technical_keyword_extraction(prompt=PROMPT_METADATA_TECHNICAL_KWD, question = question, bedrock_client=bedrock_client)\n",
    "print(kwds)\n",
    "\n",
    "print(f\"Total Input Token: {input_token}\")\n",
    "print(f\"Total Output Token: {output_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377a47d9-3149-434e-aeef-c04a2dfe408f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Functions to get the LLM response in desired format\n",
    "def llm_ouput_to_json(llm_output):\n",
    "    # Use regular expressions to find content between curly braces\n",
    "    pattern = r\"\\{([^}]*)\\}\"\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "    if len(matches) < 1:\n",
    "        return \"\", []\n",
    "    try:\n",
    "        json_obj = json.loads(\"{\" + matches[0] + \"}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {str(e)}\")\n",
    "        return \"\", []\n",
    "    return (\n",
    "        json_obj[\"rephrased_question\"],\n",
    "        json_obj[\"technical_keywords\"],\n",
    "        json_obj[\"company_keywords\"] if \"company_keywords\" in json_obj.keys() else [],\n",
    "    )\n",
    "\n",
    "def convert_quarter_format(input_string):\n",
    "    # Ensure both quarter formats are included eg: Q2'22 <--> Q2 2022\n",
    "\n",
    "    # Use regular expression to match \"Q<quarter>'<year>\"\n",
    "    match = re.match(r\"Q(\\d)\\'(\\d{2})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        quarter = match.group(1)\n",
    "        year = \"20\" + match.group(2)\n",
    "        return f\"Q{quarter} {year}\"\n",
    "\n",
    "    match = re.match(r\"Q(\\d) (\\d{4})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        quarter = match.group(1)\n",
    "        year = match.group(2)[-2:]\n",
    "        return f\"Q{quarter}'{year}\"\n",
    "\n",
    "    # For case of e.g., Q1 F2023\n",
    "    match = re.match(r\"Q(\\d) F(\\d{4})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        quarter = match.group(1)\n",
    "        year = match.group(2)[-2:]\n",
    "        return f\"Q{quarter}'{year}\"\n",
    "\n",
    "    # For case of e.g., F2023\n",
    "    match = re.match(r\"F(\\d{4})\", input_string.strip())\n",
    "\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        return f\"{year}\"\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef872a39-6cc8-4f74-8c65-856f931d3fe8",
   "metadata": {},
   "source": [
    "#### Combined Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f577bd70-d88a-4dea-9c5f-4f27a555520c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022']\n",
      "['3M']\n",
      "Total Input Token: 393\n",
      "Total Output Token: 118\n"
     ]
    }
   ],
   "source": [
    "def metadata_extraction(prompt, question, bedrock_client, time_kwds):\n",
    "    prompt_format = prompt.format(query=question, most_recent_quarter=\"Q1'24\", time_kwds=time_kwds)\n",
    "    body = json.dumps({\n",
    "        # \"prompt\": prompt_format,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\":[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt_format\n",
    "            }]\n",
    "        }],\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 1,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0,\n",
    "        \"stop_sequences\": [\"Human\"]\n",
    "    })\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId = exp_config['generation_llm_model'],\n",
    "        accept = 'application/json',\n",
    "        contentType = 'application/json',\n",
    "        body=body\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    llm_output = response_body.get('content')[0]['text']\n",
    "    input_token = int(response_body.get('usage')['input_tokens'])\n",
    "    output_token = int(response_body.get('usage')['output_tokens'])\n",
    "    rephrased_q, kwds, company_keywords = llm_ouput_to_json(llm_output)\n",
    "    time_kwds = time_kwds.split(\",\") if type(time_kwds) == str else time_kwds\n",
    "    time_kwds = [time_kwd.strip() for time_kwd in time_kwds]\n",
    "\n",
    "    new_time_kwds = []\n",
    "    for time_kwd in time_kwds:\n",
    "        new_time_kwd = convert_quarter_format(time_kwd)\n",
    "        if new_time_kwd:\n",
    "            new_time_kwds.append(new_time_kwd)\n",
    "    time_kwds.extend(new_time_kwds)\n",
    "    time_kwds = list(set(time_kwds))\n",
    "\n",
    "    kwds = list(set(kwds))\n",
    "    doc_type = []\n",
    "    if len(time_kwds) == 0 or time_kwds == [\"\"] or time_kwds == \"none\" or time_kwds is None:\n",
    "        time_kwds = [2022,2023,2024]\n",
    "    return company_keywords, time_kwds, kwds , rephrased_q, input_token, output_token \n",
    "company_keywords, time_kwds, kwds, rephrased_q, input_token, output_token  = metadata_extraction(prompt=PROMPT_METADATA_AND_QUERY_ReWr, question =question, bedrock_client =bedrock_client, time_kwds=time_kwds)\n",
    "print(time_kwds)\n",
    "print(company_keywords)\n",
    "print(f\"Total Input Token: {input_token}\")\n",
    "print(f\"Total Output Token: {output_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c5236b-eee4-40fc-98c2-78ab90f48bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022']\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions to get the LLM response in desired format\n",
    "def parse_time_kwds(time_kwds) -> list:\n",
    "    \"\"\"Given LLM generated time keywords, extract quarter and year\n",
    "\n",
    "    Args:\n",
    "        time_kwds (list): LLM generated time keywords\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with year and quarter\n",
    "    \"\"\"\n",
    "    set_tuple = set([])\n",
    "    for kwd in time_kwds:\n",
    "        match = re.match(r\"Q(\\d)\\'(\\d{2})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"q\" + match.group(1)\n",
    "            year = \"20\" + match.group(2)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "        match = re.match(r\"Q(\\d) (\\d{4})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"q\" + match.group(1)\n",
    "            year = match.group(2)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "        match = re.match(r\"Q(\\d) F(\\d{4})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"q\" + match.group(1)\n",
    "            year = match.group(2)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "        match = re.search(r\"(20\\d{2})\", kwd.strip())\n",
    "        if match:\n",
    "            quarter = \"\"\n",
    "            year = match.group(1)\n",
    "            set_tuple.add((year, quarter))\n",
    "            continue\n",
    "\n",
    "    return list(set_tuple)\n",
    "\n",
    "def get_list_variables(company_kwds, time_kwds, kwds):\n",
    "        time_kwds_tuples = parse_time_kwds(time_kwds)\n",
    "        time_key_in_years = list(set([x[0] for x in time_kwds_tuples]))\n",
    "        time_count = len(time_key_in_years)\n",
    "        q_kwds = []\n",
    "\n",
    "        return time_key_in_years, q_kwds\n",
    "\n",
    "time_key_in_years, q_kwds = get_list_variables(company_keywords, time_kwds, kwds)\n",
    "\n",
    "print(time_key_in_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe07391-7ac1-4b5b-98da-d207db962d5c",
   "metadata": {},
   "source": [
    "#### Chunks Retreival from Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58baca5c-989b-4272-a6e2-ad39d585c584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_context(q, q_kwds, time_keyword_type, time_key_in_years, kwds, time_kwds, company_kwds):\n",
    "    doc_type = []\n",
    "    contexts = []\n",
    "    retriever_embb = OpenSearchRetrieval(exp_config['opensearch_host'], exp_config['index_name_embb'])\n",
    "    embedding = get_titan_text_embedding(q)\n",
    "    contexts_sem = retriever_embb.retrieve_semantic(\n",
    "        exp_config['emb_name'],\n",
    "        embedding,\n",
    "        time_kwds,\n",
    "        company_kwds,\n",
    "        doc_type,\n",
    "        q_kwds,\n",
    "        top_k=exp_config['top_k_retrieval'],\n",
    "        use_company_kwds=True,\n",
    "        use_doc_type=False,\n",
    "    )\n",
    "\n",
    "    retriever_text = OpenSearchRetrieval(exp_config['opensearch_host'], exp_config['index_name_embb'])\n",
    "    contexts_text = retriever_text.retrieve_text(\n",
    "        kwds,\n",
    "        time_kwds,\n",
    "        company_kwds,\n",
    "        doc_type,\n",
    "        exp_config['top_k_retrieval'],\n",
    "        True,\n",
    "        False,\n",
    "    )\n",
    "    contexts.extend(\n",
    "        [\n",
    "            context[\"paragraph\"]\n",
    "            for context in results_fusion([contexts_sem, contexts_text], [0.7, 0.3], top_k=exp_config['top_k_retrieval'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd4faeb-87b5-486f-8f7f-bc33c7db6462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using exact matching in semantic search with ['2022'], ['3M']\n",
      "Using exact matching in text search with ['2022'], ['3M']\n"
     ]
    }
   ],
   "source": [
    "contexts = get_context( rephrased_q, q_kwds, time_keyword_type, time_key_in_years, kwds, time_kwds, company_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380f9ce-b291-48c3-838a-c2d8cc1c3a1d",
   "metadata": {},
   "source": [
    "### Rerank the Retreived Chunks\n",
    "##### Keyword based Reranker used for this Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b37efb24-cf23-462e-abef-526441346ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rerank_context(rephrased_query, kwds_list, time_kwds_list, company_kwds_list, contexts):\n",
    "    # Set the top k ranking based on time keywords\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(\"With keyword reranker\")\n",
    "    # Rank the contexts\n",
    "    ranker = SearchRanker()\n",
    "    combined_kwds = []\n",
    "    combined_kwds.extend(set(kwds_list))\n",
    "    combined_kwds.extend(set(time_kwds_list))\n",
    "    combined_kwds.extend(set(company_kwds_list))\n",
    "    ranked_contexts = ranker.rank_by_word_frequency(combined_kwds, contexts)\n",
    "    \n",
    "    top_k_contexts = ranked_contexts[: exp_config['top_k_ranking']]\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"**** Time taken for reranking {end_time - t0}\")\n",
    "    return ranked_contexts, top_k_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d53e9c2-4758-470d-bd54-230533190598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0034983158111572266\n"
     ]
    }
   ],
   "source": [
    "ranked_contexts, top_k_contexts = rerank_context(rephrased_q, q_kwds,time_kwds, company_keywords, contexts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61dde52f-619d-447b-8cf0-b383f500116f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks retreived = 30\n",
      "Number of Ranked Chunks to be sent to llm = 20\n"
     ]
    }
   ],
   "source": [
    "#Check point to see if contexts/chunks are retreived\n",
    "print (f\"Number of Chunks retreived = {len(ranked_contexts)}\")\n",
    "print (f\"Number of Ranked Chunks to be sent to llm = {len(top_k_contexts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891bef1-bef6-4d3d-9a61-2f2e0adc8edf",
   "metadata": {},
   "source": [
    "#### Prompt to generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eac0bdf-038d-42de-b708-403587b32ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_ANS_GENERATION = \"\"\"\n",
    "\\n\\n\n",
    "Human:\n",
    "You a financial analyst that looks at a user question, related time and technical keywords and potentially relevant context.\n",
    "\n",
    "<financial_information>\n",
    "Paragraphs related to yearly and quarterly document reports: {context} \\n\n",
    "Financial Question related to yearly and Quarterly document Reports: {query} \\n\n",
    "Same financial question written in a different way: {rephrased_query} \\n\n",
    "Related Time Keywords: {time_kwds} \\n\n",
    "</financial_information>\n",
    "\n",
    "\n",
    "<instruction>\n",
    "To answer the question, think step by step:\n",
    "1. Carefully read the question and any provided context paragraphs to find all the related paragraphs. Prioritize context paragraphs with csv tables.\n",
    "\n",
    "2. If needed, analyze financial trends and quarter-over-quarter (Q/Q) performance over the detected time spans. Calculate rates of change between quarters to identify growth or decline.\n",
    "\n",
    "3. Perform any required calculations to get the final answer, such as sums or divisions. Show the math.\n",
    "\n",
    "4. Provide a complete, correct answer based on the given information. If information is missing, state what is needed to answer the question fully.\n",
    "\n",
    "5. Present numerical values in rounded format using easy-to-read units.\n",
    "\n",
    "6. Do not answer the question with \"Based on the provided context\" or anything similar. Just providing answer is enough.\n",
    "\n",
    "7. Include the answer in a separate <answer></answer> tag with relevant and exhaustive information across all contexts. Substantiate your answer with explanation grounded in context. Conclude with a precise, concise, honest, and to-the-point answer.\n",
    "\n",
    "8. Add the page source and number to <pages></pages> tag.\n",
    "\n",
    "9. Add all Source Files from where the contexts were  used to generate the answers under a single <src></src> tag.\n",
    "</instruction>\n",
    "\n",
    "\n",
    "\\n\\n\n",
    "Assistant:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05933ebb-8d42-47c1-b965-b4f86fd48f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans_gen_prompt = PROMPT_ANS_GENERATION.format(\n",
    "            query=question,\n",
    "            context=top_k_contexts,\n",
    "            time_kwds=time_kwds,\n",
    "            rephrased_query=rephrased_q,\n",
    "            most_recent_quarter=\"Q1'24\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3349d04-8d73-4a29-bb4a-2d110d56890d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answer Generation Using Claude LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "322fea5c-e909-47b9-b7f5-c6ba8061394e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_answer(model_id: str, max_tokens: int = 800, temperature: float = 0.1, prompt: str = None):\n",
    "\n",
    "    if model_id != \"anthropic.claude-3-haiku-20240307-v1:0\":\n",
    "        body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"top_k\": 250,\n",
    "            \"top_p\": 1,\n",
    "            \"max_tokens_to_sample\": max_tokens,\n",
    "            \"temperature\": 0,\n",
    "            \"stop_sequences\": [\"Question\"]\n",
    "        })\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId = 'anthropic.claude-instant-v1',\n",
    "            accept = 'application/json',\n",
    "            contentType = 'application/json',\n",
    "            body=body\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body.get('completion')\n",
    "    else:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"messages\":[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }]\n",
    "                }],\n",
    "                \"top_k\": 50,\n",
    "                \"top_p\": 0.1,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop_sequences\": [\"Human\"],\n",
    "            }\n",
    "        )\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=body,\n",
    "            accept = 'application/json',\n",
    "            contentType = 'application/json'\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        input_token = int(response_body.get('usage')['input_tokens'])\n",
    "        output_token = int(response_body.get('usage')['output_tokens'])\n",
    "        # print(response_body)\n",
    "        # text\n",
    "        return response_body.get('content')[0]['text'], input_token, output_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e71232c-66d6-4b04-abd9-60d47a03f47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to get the LLM response in desired format\n",
    "def parse_generation(llm_prediction):\n",
    "    soup = BeautifulSoup(llm_prediction, \"html.parser\")\n",
    "    answer = soup.find(\"answer\".lower())\n",
    "    page_source = soup.find(\"pages\".lower())\n",
    "    source = soup.find(\"src\".lower())\n",
    "\n",
    "    answer = re.sub(\"<[^<]+>\", \"\", str(answer))\n",
    "    page_source = re.sub(\"<[^<]+>\", \"\", str(page_source))\n",
    "    source = re.sub(\"<[^<]+>\", \"\", str(source))\n",
    "\n",
    "    return answer+'\\n'+page_source, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00d7ac4a-ae9c-4375-943e-632b1f022717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to the financial information provided, 3M reported total net sales of $32,765 million in 2022.\n",
      "\n",
      "The key evidence is from the following paragraphs:\n",
      "27\n",
      "\"Net Sales: Refer to the preceding \"Overview\" section and the \"Performance by Business Segment\" section later in MD&amp;A for additional discussion of sales change.\"\n",
      "19\n",
      "\"3M manages its operations in four operating business segments: Safety and Industrial; Transportation and Electronics; Health Care; and Consumer.\"\n",
      "\n",
      "The total net sales figure of $32,765 million for 3M in 2022 is directly stated in the quarterly data table on 127.\n",
      "\n",
      "27\n",
      "3M_2022_10K\n",
      "Total Input Token: 14825\n",
      "Total Output Token: 179\n"
     ]
    }
   ],
   "source": [
    "prediction, input_token, output_token = get_llm_answer(model_id=exp_config['generation_llm_model'], max_tokens=4096, temperature=0.1, prompt=ans_gen_prompt)\n",
    "answer, source = parse_generation(prediction)\n",
    "print(answer)\n",
    "print(source)\n",
    "print(f\"Total Input Token: {input_token}\")\n",
    "print(f\"Total Output Token: {output_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7299352-53f4-4040-a233-a6486ca1e0fa",
   "metadata": {},
   "source": [
    "### Bulk Processing - Proceed ONLY After Indexing is completed for all the files in notebook 01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f45287-2401-4448-b2dc-f3d137dc3ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\"What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\",\n",
    "             \"Is 3M a capital-intensive business based on FY2022 data?\",\n",
    "             \"Does Adobe have an improving operating margin profile as of FY2022? If operating margin is not a useful metric for a company like this, then state that and explain why.\",\n",
    "             \"Does Adobe have an improving Free cashflow conversion as of FY2022?\",\n",
    "             \"Answer the following question as if you are an equity research analyst and have lost internet connection so you do not have access to financial metric providers. According to the details clearly outlined within the P&L statement and the statement of cash flows, what is the FY2015 depreciation and amortization (D&A from cash flow statement) % margin for AMD?\",\n",
    "             \"From FY21 to FY22, excluding Embedded, in which AMD reporting segment did sales proportionally increase the most?\",\n",
    "             \"How much has the effective tax rate of American Express changed between FY2021 and FY2022?\",\n",
    "             \"What was the largest liability in American Express's Balance Sheet in 2022?\",\n",
    "             \"What is the year end FY2019 total amount of inventories for Best Buy? Answer in USD millions. Base your judgments on the information provided primarily in the balance sheet.\",\n",
    "             \"Are Best Buy's gross margins historically consistent (not fluctuating more than roughly 2% each year)? If gross margins are not a relevant metric for a company like this, then please state that and explain why.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7743195-e7ed-449b-989b-cf5b2b6343be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using exact matching in semantic search with ['2018'], ['3M']\n",
      "Using exact matching in text search with ['2018'], ['3M']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.008697748184204102\n",
      "\n",
      "According to the cash flow statement in the 3M 2018 10-K report, the capital expenditure (purchases of property, plant and equipment) for fiscal year 2018 was $1,577 million.\n",
      "\n",
      "Specifically, the cash flow statement shows the following:\n",
      "\n",
      "\"Purchases of property, plant and equipment (PP&amp;E)\n",
      "2018: $1,577 million\n",
      "2017: $1,373 million\n",
      "2016: $1,420 million\"\n",
      "\n",
      "So the capital expenditure amount for 3M in fiscal year 2018 was $1,577 million.\n",
      "\n",
      "46\n",
      "3M_2018_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.93\n",
      "****Total Time taken for Metadata extraction - keywords : 0.61\n",
      "****Total Time taken for Metadata extraction - combined : 1.45\n",
      "****Total Time taken for Answer Generation : 2.48\n",
      "Using exact matching in semantic search with ['2022'], ['3M']\n",
      "Using exact matching in text search with ['2022'], ['3M']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.00782155990600586\n",
      "\n",
      "Based on the financial information provided in the 3M 2022 10-K report, 3M appears to be a capital-intensive business:\n",
      "\n",
      "- The company had $9.2 billion in net property, plant, and equipment (PP&amp;E) as of December 31, 2022, compared to total assets of $38.1 billion. This indicates that PP&amp;E makes up a significant portion (around 24%) of 3M's total assets, suggesting a capital-intensive operating model.\n",
      "\n",
      "- 3M's capital spending was $1.749 billion in 2022, indicating substantial investments in manufacturing and production capabilities to support its diverse product portfolio and global operations.\n",
      "\n",
      "- The report states that \"Investments in property, plant and equipment enable growth across many diverse markets, helping to meet product demand and increasing manufacturing efficiency.\" This further confirms 3M's reliance on capital-intensive assets to drive its business.\n",
      "\n",
      "- Additionally, the report discusses 3M's plans to continue increasing its investment in manufacturing and sourcing capability to better align its product capability with sales in major geographic areas and serve customers globally.\n",
      "\n",
      "In conclusion, based on the high level of PP&amp;E relative to total assets, significant capital expenditures, and the company's focus on manufacturing and production capabilities, 3M can be considered a capital-intensive business.\n",
      "\n",
      "38, 34\n",
      "3M_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.71\n",
      "****Total Time taken for Metadata extraction - keywords : 0.50\n",
      "****Total Time taken for Metadata extraction - combined : 1.94\n",
      "****Total Time taken for Answer Generation : 5.23\n",
      "Using exact matching in semantic search with ['2022'], ['Adobe']\n",
      "Using exact matching in text search with ['2022'], ['Adobe']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.003468036651611328\n",
      "\n",
      "Based on the financial information provided in the 10-K report, Adobe does not appear to have an improving operating margin profile as of fiscal year 2022 (FY2022). The report shows that Adobe's operating expenses increased by 15% in FY2022 compared to FY2021, outpacing the 12% increase in total revenue. This suggests that Adobe's operating margin, which is not directly reported, likely declined in FY2022.\n",
      "\n",
      "The report provides the following relevant details:\n",
      "- Total operating expenses increased from $8.12 billion in FY2021 to $9.34 billion in FY2022, a 15% increase.\n",
      "- This increase in operating expenses was primarily due to increases in base and incentive compensation, as well as increased marketing spend.\n",
      "- Meanwhile, total revenue increased from $15.79 billion in FY2021 to $17.61 billion in FY2022, a 12% increase.\n",
      "\n",
      "Since operating margin is not a directly reported metric in Adobe's financial statements, it cannot be calculated precisely from the information provided. However, the disproportionate increase in operating expenses relative to revenue growth suggests that Adobe's operating margin profile likely did not improve in FY2022. Operating margin is an important metric for evaluating the profitability and efficiency of a software company like Adobe, so the lack of improvement in this area is noteworthy.\n",
      "\n",
      "In summary, based on the financial information presented, Adobe does not appear to have an improving operating margin profile as of fiscal year 2022.\n",
      "\n",
      "40, 41, 42, 43, 44\n",
      "ADOBE_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.77\n",
      "****Total Time taken for Metadata extraction - keywords : 3.74\n",
      "****Total Time taken for Metadata extraction - combined : 1.55\n",
      "****Total Time taken for Answer Generation : 4.36\n",
      "Using exact matching in semantic search with ['2022'], ['Adobe']\n",
      "Using exact matching in text search with ['2022'], ['Adobe']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0032334327697753906\n",
      "\n",
      "Based on the financial information provided in the 10-K report, Adobe Inc. does not have an improving free cash flow conversion ratio as of their Fiscal Year 2022 (FY2022).\n",
      "\n",
      "The report shows that Adobe's free cash flow conversion ratio declined from 99% in FY2021 to 66% in FY2022. This indicates that Adobe's free cash flow generation has decreased relative to its net income in FY2022 compared to the prior year.\n",
      "\n",
      "The key details are:\n",
      "- Free cash flow in FY2022 was $3,842 million\n",
      "- Net income attributable to 3M in FY2022 was $5,777 million\n",
      "- Free cash flow conversion ratio = Free cash flow / Net income = $3,842 million / $5,777 million = 66%\n",
      "- This ratio declined from 99% in FY2021, indicating a worsening of Adobe's free cash flow conversion.\n",
      "\n",
      "So in summary, Adobe's free cash flow conversion ratio declined in FY2022 compared to the prior year, suggesting their free cash flow generation has not kept pace with their net income growth.\n",
      "\n",
      "41, 47\n",
      "ADOBE_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.89\n",
      "****Total Time taken for Metadata extraction - keywords : 0.58\n",
      "****Total Time taken for Metadata extraction - combined : 1.38\n",
      "****Total Time taken for Answer Generation : 3.78\n",
      "Using exact matching in semantic search with ['2015'], ['AMD']\n",
      "Using exact matching in text search with ['2015'], ['AMD']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.008278369903564453\n",
      "\n",
      "According to the details in the Profit and Loss (P&amp;L) statement and the Statement of Cash Flows, the Fiscal Year (FY) 2015 Depreciation and Amortization (D&amp;A) percentage margin for Advanced Micro Devices (AMD) is 4.7%.\n",
      "\n",
      "The P&amp;L statement shows that AMD had total net revenue of $3,991 million in FY2015. The Statement of Cash Flows shows that AMD had $94 million in depreciation expense in FY2015.\n",
      "\n",
      "To calculate the D&amp;A percentage margin, I divided the depreciation expense of $94 million by the total net revenue of $3,991 million, which results in a D&amp;A percentage margin of 4.7%.\n",
      "\n",
      "68\n",
      "AMD_2015_10K\n",
      "\n",
      "68\n",
      "AMD_2015_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.82\n",
      "****Total Time taken for Metadata extraction - keywords : 0.42\n",
      "****Total Time taken for Metadata extraction - combined : 1.69\n",
      "****Total Time taken for Answer Generation : 2.93\n",
      "Using exact matching in semantic search with ['2021', '2022'], ['AMD']\n",
      "Using exact matching in text search with ['2021', '2022'], ['AMD']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.006891489028930664\n",
      "\n",
      "Based on the financial information provided, excluding the Embedded segment, the AMD reporting segment that saw the most proportional increase in sales from fiscal year 2021 (FY21) to fiscal year 2022 (FY22) was the Data Center segment.\n",
      "\n",
      "The key evidence is:\n",
      "- Data Center segment net revenue increased from $3.7 billion in FY21 to $6.0 billion in FY22, a 64% increase. [Source: AMD_2022_10K, page 48]\n",
      "- Client segment net revenue decreased from $6.9 billion in FY21 to $6.2 billion in FY22, a 10% decrease. [Source: AMD_2022_10K, page 48]\n",
      "- Gaming segment net revenue increased from $5.6 billion in FY21 to $6.8 billion in FY22, a 21% increase. [Source: AMD_2022_10K, page 49]\n",
      "\n",
      "Therefore, excluding the Embedded segment, the AMD reporting segment with the most proportional increase in sales from FY21 to FY22 was the Data Center segment, with a 64% increase.\n",
      "\n",
      "48-49\n",
      "AMD_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.99\n",
      "****Total Time taken for Metadata extraction - keywords : 0.42\n",
      "****Total Time taken for Metadata extraction - combined : 2.34\n",
      "****Total Time taken for Answer Generation : 3.48\n",
      "Using exact matching in semantic search with ['2021', '2022'], ['American Express']\n",
      "Using exact matching in text search with ['2021', '2022'], ['American Express']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.004189014434814453\n",
      "\n",
      "The effective tax rate of American Express decreased from 24.6% in fiscal year 2021 to 21.6% in fiscal year 2022, a decrease of 3.0 percentage points.\n",
      "\n",
      "According to the financial information provided:\n",
      "\n",
      "\"The effective tax rate was 21.6 percent and 24.6 percent for 2022 and 2021, respectively. The reduction in the effective tax rate primarily reflected discrete tax benefits in the current year related to the resolution of prior-year tax items.\" [Source: AMERICANEXPRESS_2022_10K, page 49]\n",
      "\n",
      "So the effective tax rate of American Express decreased by 3.0 percentage points from fiscal year 2021 to fiscal year 2022.\n",
      "\n",
      "49\n",
      "AMERICANEXPRESS_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.99\n",
      "****Total Time taken for Metadata extraction - keywords : 0.41\n",
      "****Total Time taken for Metadata extraction - combined : 6.27\n",
      "****Total Time taken for Answer Generation : 2.85\n",
      "Using exact matching in semantic search with ['2022'], ['American Express']\n",
      "Using exact matching in text search with ['2022'], ['American Express']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0034351348876953125\n",
      "\n",
      "According to the financial statements, the largest liability on American Express's balance sheet in 2022 was the Membership Rewards liability, which stood at $12,789 million as of December 31, 2022. This is reported in the \"Other Liabilities\" section of the balance sheet, as shown in the table on page 127 of the 10-K filing.\n",
      "\n",
      "127\n",
      "AMERICANEXPRESS_2022_10K\n",
      "\n",
      "127\n",
      "AMERICANEXPRESS_2022_10K\n",
      "****Total Time taken for Metadata extraction - time : 0.73\n",
      "****Total Time taken for Metadata extraction - keywords : 0.29\n",
      "****Total Time taken for Metadata extraction - combined : 1.08\n",
      "****Total Time taken for Answer Generation : 2.10\n",
      "Using exact matching in semantic search with ['2019'], ['Best Buy']\n",
      "Using exact matching in text search with ['2019'], ['Best Buy']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0032885074615478516\n",
      "\n",
      "According to the Consolidated Balance Sheets in the Best Buy 2019 10-K report, the total amount of merchandise inventories reported by Best Buy for Fiscal Year 2019 was $5,409 million.\n",
      "\n",
      "\n",
      "[source page: 52]\n",
      "\n",
      "\n",
      "BESTBUY_2019_10K\n",
      "\n",
      "****Total Time taken for Metadata extraction - time : 0.77\n",
      "****Total Time taken for Metadata extraction - keywords : 0.43\n",
      "****Total Time taken for Metadata extraction - combined : 1.36\n",
      "****Total Time taken for Answer Generation : 2.43\n",
      "Using exact matching in semantic search with ['2021', '2022', '2020', '2023'], ['Best Buy']\n",
      "Using exact matching in text search with ['2021', '2022', '2020', '2023'], ['Best Buy']\n",
      "With keyword reranker\n",
      "**** Time taken for reranking 0.0038368701934814453\n",
      "\n",
      "Based on the financial information provided in the context paragraphs, Best Buy's gross profit margins have not been historically consistent, fluctuating more than 2% each year:\n",
      "\n",
      "- In fiscal 2023, Best Buy's consolidated gross profit rate was 21.4%, down from 22.5% in fiscal 2022 and 22.4% in fiscal 2021. This represents a decline of over 1 percentage point year-over-year.\n",
      "- In the Domestic segment, the gross profit rate decreased from 23.4% in fiscal 2018 to 23.3% in fiscal 2019, a decline of 0.1 percentage points.\n",
      "- In the International segment, the gross profit rate decreased from 23.9% in fiscal 2022 to 23.0% in fiscal 2023, a decline of 0.9 percentage points.\n",
      "\n",
      "The context indicates that the decreases in gross profit rate were primarily driven by factors such as lower product margin rates, increased promotions, lower services margin rates, and higher supply chain costs. These fluctuations in gross margins suggest they are a relevant and meaningful metric for evaluating Best Buy's financial performance.\n",
      "\n",
      "In summary, Best Buy's gross profit margins have not been historically consistent, with fluctuations exceeding 2 percentage points in recent years. The gross margin metric appears to be an important indicator of the company's financial health and operational efficiency.\n",
      "\n",
      "25, 30, 32\n",
      "BESTBUY_2023_10K, BESTBUY_2019_10K\n",
      "****Total Time taken for Metadata extraction - time : 1.25\n",
      "****Total Time taken for Metadata extraction - keywords : 3.45\n",
      "****Total Time taken for Metadata extraction - combined : 2.01\n",
      "****Total Time taken for Answer Generation : 5.15\n"
     ]
    }
   ],
   "source": [
    "llm_answers =[]\n",
    "llm_contexts =[]\n",
    "latency_meta_time = []\n",
    "latency_meta_kwd = []\n",
    "latency_meta_comb = []\n",
    "latency_meta_ans_gen = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "for question in questions:    \n",
    "    t0=time.time()\n",
    "    time_keyword_type, time_kwds, explanation, input_token1, output_token1 = time_keyword_extraction(\n",
    "        prompt=PROMPT_METADATA_GENERATION_time,\n",
    "        question = question,\n",
    "        bedrock_client=bedrock_client)\n",
    "    t1=time.time()\n",
    "    kwds, input_token2, output_token2  = technical_keyword_extraction(\n",
    "        prompt=PROMPT_METADATA_TECHNICAL_KWD,\n",
    "        question = question,\n",
    "        bedrock_client=bedrock_client)\n",
    "    t2=time.time()\n",
    "    company_keywords, time_kwds, kwds, rephrased_q, input_token3, output_token3 = metadata_extraction(\n",
    "        prompt=PROMPT_METADATA_AND_QUERY_ReWr,\n",
    "        question =question,\n",
    "        bedrock_client =bedrock_client,\n",
    "        time_kwds=time_kwds)\n",
    "    t3=time.time()\n",
    "    time_key_in_years, q_kwds = get_list_variables(\n",
    "        company_keywords,\n",
    "        time_kwds,\n",
    "        kwds)\n",
    "    contexts = get_context( rephrased_q,\n",
    "                           q_kwds,\n",
    "                           time_keyword_type,\n",
    "                           time_key_in_years,\n",
    "                           kwds,\n",
    "                           time_kwds,\n",
    "                           company_keywords)\n",
    "    ranked_contexts, top_k_contexts = rerank_context(rephrased_q,\n",
    "                                                     q_kwds,time_kwds,\n",
    "                                                     company_keywords,\n",
    "                                                     contexts )\n",
    "    ans_gen_prompt = PROMPT_ANS_GENERATION.format(\n",
    "                query=question,\n",
    "                context=top_k_contexts,\n",
    "                time_kwds=time_kwds,\n",
    "                rephrased_query=rephrased_q,\n",
    "                most_recent_quarter=\"Q1'24\",\n",
    "            )\n",
    "    t4=time.time()\n",
    "    prediction, input_token4, output_token4 = get_llm_answer(model_id=exp_config['generation_llm_model'],\n",
    "                                max_tokens=4096,\n",
    "                                temperature=0.1,\n",
    "                                prompt=ans_gen_prompt)\n",
    "    t5=time.time()\n",
    "    answer, source = parse_generation(prediction)\n",
    "    total_input_token = input_token1 + input_token2 + input_token3 + input_token4\n",
    "    total_output_token = output_token1 + output_token2 + output_token3 + output_token4\n",
    "    input_tokens = input_tokens + [total_input_token]\n",
    "    output_tokens = output_tokens + [total_output_token]\n",
    "    llm_answers = llm_answers + [answer.replace(\"\\n\", \"\")]\n",
    "    llm_contexts = llm_contexts +[ranked_contexts]\n",
    "    latency_meta_time = latency_meta_time + [t1-t0]\n",
    "    latency_meta_kwd = latency_meta_kwd + [t2-t1]\n",
    "    latency_meta_comb = latency_meta_comb + [t3-t2]\n",
    "    latency_meta_ans_gen = latency_meta_ans_gen + [t5-t4]\n",
    "    print(answer)\n",
    "    print(source)\n",
    "    print(f\"****Total Time taken for Metadata extraction - time : {(t1-t0):.2f}\")\n",
    "    print(f\"****Total Time taken for Metadata extraction - keywords : {(t2-t1):.2f}\")\n",
    "    print(f\"****Total Time taken for Metadata extraction - combined : {(t3-t2):.2f}\")\n",
    "    print(f\"****Total Time taken for Answer Generation : {(t5-t4):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b2243-669b-475b-b404-38833d7dbeca",
   "metadata": {},
   "source": [
    "### Write to CSV for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffef7c59-e739-491d-b3be-b16c06582905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Open the input CSV file for reading\n",
    "with open('../data/selected_samples.csv', 'r') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    # Open the output CSV file for writing\n",
    "    fieldnames = reader.fieldnames  # Get the fieldnames from the input file\n",
    "    \n",
    "    # Add the new column names to the fieldnames list\n",
    "    new_columns = ['latency_meta_time', 'latency_meta_kwd', 'latency_meta_comb', 'latency_meta_ans_gen', 'input_tokens', 'output_tokens']\n",
    "    fieldnames.extend(new_columns)\n",
    "    \n",
    "    # # Open the output CSV file for writing\n",
    "    # fieldnames = reader.fieldnames  # Get the fieldnames from the input file\n",
    "    with open('../outputs/rag_outputs/output_claude.csv', 'w', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        # Lists of new values to be updated in the specified columns\n",
    "        llm_ans = llm_answers\n",
    "        llm_contex = llm_contexts\n",
    "        \n",
    "        # Iterate over the rows in the input file and the new values\n",
    "        for row_index, row in enumerate(reader, start=1):\n",
    "            # Update the values in the desired columns for the current row\n",
    "            if row_index <= len(llm_ans):\n",
    "                row['llm_answer'] = llm_ans[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['llm_contexts'] = llm_contex[row_index - 1]\n",
    "                \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_time'] = latency_meta_time[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_kwd'] = latency_meta_kwd[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_comb'] = latency_meta_comb[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['latency_meta_ans_gen'] = latency_meta_ans_gen[row_index - 1]\n",
    "            if row_index <= len(llm_contex):\n",
    "                row['input_tokens'] = input_tokens[row_index - 1]\n",
    "            \n",
    "            if row_index <= len(llm_contex):\n",
    "                row['output_tokens'] = output_tokens[row_index - 1]\n",
    "            \n",
    "            # Write the updated row to the output file\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91114f51-62c9-4fc0-b72c-4036596661e3",
   "metadata": {},
   "source": [
    "### <center>----------EOF---------</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
