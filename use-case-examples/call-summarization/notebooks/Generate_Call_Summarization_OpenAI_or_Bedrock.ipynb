{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcad037",
   "metadata": {},
   "source": [
    "# Generate Call Summarization OpenAI or Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3109e7",
   "metadata": {},
   "source": [
    "This notebook goes over how to generate individual call metadata (also known as Gen AI Powered Fields) from call transcripts. Primarily these include: \n",
    "\n",
    "- Summary\n",
    "- Topic\n",
    "- Root Cause\n",
    "- Issue Resolved (Y/N)\n",
    "- Callback (Y/N)\n",
    "- Next Steps by the Customer and Agent\n",
    "\n",
    "for each call. \n",
    "More metadata fields can be added as per requirement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a48b0",
   "metadata": {},
   "source": [
    "We generate summaries for both Source Model as well as Target Model. Our source model by default is \"mistral.mistral-large-2402-v1:0\" but it can be changed to any other model in the config.py file under /src folder. This notebook also demonstrates using OpenAI as the source model. If you have the OpenAI key, you can invoke the model to generate summaries using OpenAI. As for the Target model, this workshop is designed to work with \"anthropic.claude-3-sonnet-20240229-v1:0\" (Step 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ae2a2",
   "metadata": {},
   "source": [
    "![1a3a_Notebook.png](../images/1a3a_Notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f3bcb",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "We start with installing deepeval, openai and boto3 libraries\n",
    "Run below cell. You can ignore pip errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2c23b-97c7-462b-b264-b20d7ed40bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pydantic==1.10.8 --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install -U boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d0f8c-0868-4aa1-aa13-f2cb4ef1d981",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e9db4",
   "metadata": {},
   "source": [
    "We first define our parameters for source and target models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5478802d-2dbc-4092-8cb6-230072e69544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id=mistral.mistral-large-2402-v1:0, prompt_id=raw\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "from config import *\n",
    "\n",
    "# src models\n",
    "model_id = MISTRAL_MODEL_ID\n",
    "# model_id = CLAUDE_MODEL_ID\n",
    "# model_id = LAMA_MODEL_ID\n",
    "# model_id = OPENAI_MODEL_ID\n",
    "\n",
    "# OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# target models\n",
    "# model_id = CLAUDE_MODEL_ID\n",
    "# model_id = LAMA_MODEL_ID\n",
    "# model_id = MISTRAL_MODEL_ID\n",
    "\n",
    "# summarization prompt\n",
    "prompt_id = \"raw\"\n",
    "# prompt_id=\"optimized\" # use for CLAUDE_MODEL_ID\n",
    "\n",
    "print(\"model_id=%s, prompt_id=%s\" % (model_id, prompt_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c194a8-c5c6-4044-8c94-e8fad91275a3",
   "metadata": {},
   "source": [
    "### Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca032bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import base64\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae8ac2-f2cc-4546-b52b-f0b6d7b3dff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenAI invokation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349b635",
   "metadata": {},
   "source": [
    "This code creates a definition for invoking OpenAI function to generate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eef81b0-8f94-4406-9c40-d10b06335af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def invoke_openai_base(\n",
    "    openai_client, messages, model_id, max_tokens=1024, temperature=0.0\n",
    "):\n",
    "    time0 = time.time()\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    generated_text = completion.choices[0].message.content\n",
    "\n",
    "    input_tokens = completion.usage.prompt_tokens\n",
    "    output_tokens = completion.usage.completion_tokens\n",
    "\n",
    "    end_time = time.time() - time0\n",
    "    latency_end = end_time\n",
    "\n",
    "    output_obj = {\n",
    "        \"response_text\": generated_text,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"latency_end\": latency_end,\n",
    "    }\n",
    "\n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec8623-f61a-4fe4-82ea-5cb4f7c9ed6e",
   "metadata": {},
   "source": [
    "Test the invokation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abae6bdf-1694-4df8-873b-d44fb3e7f79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what is LLM (Large Language Model)?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "if os.environ[\"OPENAI_API_KEY\"] != \"\":\n",
    "    answer = invoke_openai_base(openai_client, messages, OPENAI_MODEL_ID)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09f3f3-6b19-46f3-afd4-85f2d544d945",
   "metadata": {},
   "source": [
    "### Bedrock invokation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f379c",
   "metadata": {},
   "source": [
    "Initialize the Amazon Bedrock runtime client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f626534b-9b79-4cd5-9683-6df8a77ab252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_config = Config(\n",
    "    region_name=AWS_REGION,\n",
    "    signature_version=\"v4\",\n",
    "    retries={\"max_attempts\": 3, \"mode\": \"standard\"},\n",
    ")\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", config=my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54929f7",
   "metadata": {},
   "source": [
    "Create definition for the invocation function for models in Bedrock. The invocation uses the Converse API for both text and streaming text data. We also calculate metrics like Latency, Input Tokens and Output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d214041e-a8fd-4a6d-a1a2-4e67b362aae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_base(\n",
    "    client,\n",
    "    messages=[{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}],\n",
    "    system=\"You are an assistant.\",\n",
    "    model_id=\"\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    stop_sequences=[\"Human:\"],\n",
    "    use_streaming=False,\n",
    "    print_details=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Invokes Bedrock models to run an inference using the input\n",
    "    provided in the request body.\n",
    "\n",
    "    :param prompt: The prompt that you want to complete.\n",
    "    :return: Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke Claude models with the text prompt\n",
    "\n",
    "    inference_config = {\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    system_config = []\n",
    "\n",
    "    if top_p is not None:\n",
    "        inference_config[\"topP\"] = top_p\n",
    "    if stop_sequences is not None:\n",
    "        inference_config[\"stopSequences\"] = stop_sequences\n",
    "\n",
    "    if system is not None:\n",
    "        system_config.append({\"text\": system})\n",
    "\n",
    "    time0 = time.time()\n",
    "    if use_streaming:\n",
    "        response = client.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config,\n",
    "            system=system_config,\n",
    "        )\n",
    "\n",
    "        stream = response[\"stream\"]\n",
    "        output_text = \"\"\n",
    "        la = True\n",
    "        if stream:\n",
    "            for chunk in stream:\n",
    "                if la:\n",
    "                    start_time = time.time() - time0\n",
    "                    la = False\n",
    "                if \"contentBlockDelta\" in chunk:\n",
    "                    text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "                    print(text, end=\"\")\n",
    "                    output_text = output_text + text\n",
    "                if \"metadata\" in chunk:\n",
    "                    input_tokens = chunk[\"metadata\"][\"usage\"][\"inputTokens\"]\n",
    "                    output_tokens = chunk[\"metadata\"][\"usage\"][\"outputTokens\"]\n",
    "                    latency_start = chunk[\"metadata\"][\"metrics\"][\"latencyMs\"] / 1000\n",
    "\n",
    "        end_time = time.time() - time0\n",
    "        latency_end = end_time\n",
    "        output_list = [output_text]\n",
    "        print(f\"\\n**** Stream End {end_time} ****\\n\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config,\n",
    "            system=system_config,\n",
    "        )\n",
    "\n",
    "        end_time = time.time() - time0\n",
    "        latency_start = end_time\n",
    "        latency_end = end_time\n",
    "\n",
    "        # Process and print the response\n",
    "        result = response.get(\"output\")\n",
    "        input_tokens = response[\"usage\"][\"inputTokens\"]\n",
    "        output_tokens = response[\"usage\"][\"outputTokens\"]\n",
    "        output_list = result[\"message\"].get(\"content\", [])\n",
    "        output_text = \"\\n\".join([x[\"text\"] for x in output_list])\n",
    "        if print_details:\n",
    "            print(f\"Response(s):\")\n",
    "            print(output_text)\n",
    "\n",
    "    if print_details:\n",
    "        print(\"Latency details:\")\n",
    "        print(f\"- The start latency is {latency_start} seconds.\")\n",
    "        print(f\"- The full invocation latency is {latency_end} seconds.\")\n",
    "\n",
    "        print(\"Invocation details:\")\n",
    "        print(f\"- The input length is {input_tokens} tokens.\")\n",
    "        print(f\"- The output length is {output_tokens} tokens.\")\n",
    "\n",
    "    output_obj = {\n",
    "        \"response_text\": output_text,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"latency_start\": latency_start,\n",
    "        \"latency_end\": latency_end,\n",
    "    }\n",
    "\n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc55818-00a3-4b72-a91a-20539242ae71",
   "metadata": {},
   "source": [
    "Test the invokation\n",
    "You can change parameters like temperature, Top K, Top P as per the requirements. You can also alter the value of \"use_streaming\" to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01177da9-c0e7-4349-b6a4-396a03fc10da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(s):\n",
      "YouTube is a video sharing platform that was created in 2005 by three former PayPal employees: Chad Hurley, Steve Chen, and Jawed Karim. The idea for YouTube was born out of the difficulty the three founders faced while trying to share videos of a dinner party online. At the time, there were no easy-to-use video sharing platforms available, so they decided to create their own.\n",
      "\n",
      "YouTube was officially launched in February 2005, and the first video, titled \"Me at the zoo,\" was uploaded by Karim on April 23, 2005. The video, which is 18 seconds long and features Karim at the San Diego Zoo, is still available on the site.\n",
      "\n",
      "In the early days, YouTube was primarily used by individuals to share personal videos, but it quickly grew in popularity and began to attract a wider range of content creators, including musicians, comedians, and filmmakers. In 2006, Google acquired YouTube for $1.65 billion, and the platform has since become one of the most popular websites in the world, with billions of users and millions of hours of video content uploaded every day.\n",
      "\n",
      "Over the years, YouTube has introduced a number of new features and tools to improve the user experience and help content creators reach larger audiences. These have included the introduction of the partner program, which allows creators to monetize their videos through advertising, as well as the launch of YouTube TV, a live TV streaming service.\n",
      "\n",
      "Today, YouTube is a global platform that is used by people all over the world to discover, watch, and share videos on a wide range of topics. It has become an important platform for creative expression, education, and entertainment, and it continues to evolve and grow as new technologies and trends emerge.\n",
      "Latency details:\n",
      "- The start latency is 9.70946192741394 seconds.\n",
      "- The full invocation latency is 9.70946192741394 seconds.\n",
      "Invocation details:\n",
      "- The input length is 12 tokens.\n",
      "- The output length is 392 tokens.\n",
      "YouTube is a video sharing platform that was created in 2005 by three former PayPal employees: Chad Hurley, Steve Chen, and Jawed Karim. The idea for YouTube was born out of the difficulty the three founders faced while trying to share videos of a dinner party online. At the time, there were no easy-to-use video sharing platforms available, so they decided to create their own.\n",
      "\n",
      "YouTube was officially launched in February 2005, and the first video, titled \"Me at the zoo,\" was uploaded by Karim on April 23, 2005. The video, which is 18 seconds long and features Karim at the San Diego Zoo, is still available on the site.\n",
      "\n",
      "In the early days, YouTube was primarily used by individuals to share personal videos, but it quickly grew in popularity and began to attract a wider range of content creators, including musicians, comedians, and filmmakers. In 2006, Google acquired YouTube for $1.65 billion, and the platform has since become one of the most popular websites in the world, with billions of users and millions of hours of video content uploaded every day.\n",
      "\n",
      "Over the years, YouTube has introduced a number of new features and tools to improve the user experience and help content creators reach larger audiences. These have included the introduction of the partner program, which allows creators to monetize their videos through advertising, as well as the launch of YouTube TV, a live TV streaming service.\n",
      "\n",
      "Today, YouTube is a global platform that is used by people all over the world to discover, watch, and share videos on a wide range of topics. It has become an important platform for creative expression, education, and entertainment, and it continues to evolve and grow as new technologies and trends emerge.\n"
     ]
    }
   ],
   "source": [
    "if model_id != OPENAI_MODEL_ID:\n",
    "    question = \"What is the history of Youtube?\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": question}]},\n",
    "    ]\n",
    "\n",
    "    output_obj = invoke_base(\n",
    "        client,\n",
    "        messages=messages,\n",
    "        system=None,\n",
    "        model_id=model_id,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        stop_sequences=[\"Human:\"],\n",
    "        use_streaming=False,\n",
    "        print_details=True,\n",
    "    )\n",
    "\n",
    "    answer = output_obj[\"response_text\"]\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b1674",
   "metadata": {},
   "source": [
    "### Load the Transcripts Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2524699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcripts = pd.read_csv(\"../data/call_transcripts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "109a8c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>call_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C101</td>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>3/5/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C102</td>\n",
       "      <td>2</td>\n",
       "      <td>A02</td>\n",
       "      <td>Agent: Good morning, thank you for calling SB ...</td>\n",
       "      <td>3/11/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C103</td>\n",
       "      <td>3</td>\n",
       "      <td>A03</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>2/29/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C104</td>\n",
       "      <td>4</td>\n",
       "      <td>A04</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>3/7/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C105</td>\n",
       "      <td>5</td>\n",
       "      <td>A05</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>2/21/24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  call_id agent_id  \\\n",
       "0        C101        1      A01   \n",
       "1        C102        2      A02   \n",
       "2        C103        3      A03   \n",
       "3        C104        4      A04   \n",
       "4        C105        5      A05   \n",
       "\n",
       "                                          transcript     date  \n",
       "0  \\nAgent: Good morning, thank you for calling S...   3/5/24  \n",
       "1  Agent: Good morning, thank you for calling SB ...  3/11/24  \n",
       "2  \\nAgent: Good morning, thank you for calling S...  2/29/24  \n",
       "3  \\nAgent: Good morning, thank you for calling S...   3/7/24  \n",
       "4  \\nAgent: Good morning, thank you for calling S...  2/21/24  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382aa220-666c-4a6f-a5e7-e05a47c562f1",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e104ec",
   "metadata": {},
   "source": [
    "In this section, the user can add their own prompts to */data/call_summarization_prompts.csv* to generate the corresponding fields.\n",
    "\n",
    "In this example, we generate only the \"summary\" of the call. You can add other field in the field_to_question_map dictionary below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3e9c4d5-d463-4e87-9218-c1cbdbdbf258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>Answer the question below, by obtaining the {F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>optimized</td>\n",
       "      <td>You will be answering a question by obtaining ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                                        prompt_text\n",
       "0        raw  Answer the question below, by obtaining the {F...\n",
       "1  optimized  You will be answering a question by obtaining ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = pd.read_csv(\"../data/call_summarization_prompts.csv\", encoding=\"UTF-8\")\n",
    "prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cb1fc94-719f-4fa1-9770-98c32b10086f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': {'prompt_text': \"Answer the question below, by obtaining the {FIELD} from following call transcript of a call between a customer and a customer agent on a specific issue. \\nIf you cannot answer the question, reply with 'n/a'. Use gender neutral pronouns.\\nWhen you reply, do not use XML tags in the answer. Respond with the answer and a 1-2 sentence explanation. \\n\\nQuestion: {QUESTION}\\n\\nTranscript:\\n{TRANSCRIPT}\\n\\nAnswer in the following way: \\nAnswer: <your answer to the question>\\nStep by Step Reason: <corresponding reason>\"},\n",
       " 'optimized': {'prompt_text': 'You will be answering a question by obtaining information from a transcript of a call between a customer and a customer service agent. Here are the steps:\\n\\n1. Read the provided transcript carefully:\\n<transcript>\\n{TRANSCRIPT}\\n</transcript>\\n\\n2. Identify the {FIELD} from the transcript that is relevant to answering the question: \"{QUESTION}\"\\n\\n3. Provide your answer and reasoning in the following format:\\n\\nAnswer: <your answer to the question>\\nStep by Step Reason: <explain how you arrived at your answer by referencing relevant parts of the transcript>\\n\\nIf you cannot determine an answer from the transcript, simply respond with:\\nAnswer: n/a\\nStep by Step Reason: The transcript does not contain enough information to answer the question.\\n\\nDo not use any XML tags in your response besides the formatting shown above. Use gender neutral pronouns when referring to people mentioned in the transcript.'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dict = prompts.set_index(\"prompt_id\").T.to_dict()\n",
    "prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae0d44b0-357f-4588-8522-4e36984667ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question below, by obtaining the {FIELD} from following call transcript of a call between a customer and a customer agent on a specific issue. \n",
      "If you cannot answer the question, reply with 'n/a'. Use gender neutral pronouns.\n",
      "When you reply, do not use XML tags in the answer. Respond with the answer and a 1-2 sentence explanation. \n",
      "\n",
      "Question: {QUESTION}\n",
      "\n",
      "Transcript:\n",
      "{TRANSCRIPT}\n",
      "\n",
      "Answer in the following way: \n",
      "Answer: <your answer to the question>\n",
      "Step by Step Reason: <corresponding reason>\n"
     ]
    }
   ],
   "source": [
    "user_prompt_template_raw = prompt_dict[prompt_id][\"prompt_text\"]\n",
    "print(user_prompt_template_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf609f",
   "metadata": {},
   "source": [
    "You can enter as many metadata fields below that you want to generate in the output file. We are using the ones mentioned in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83c4c84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_to_question_map = {\n",
    "    \"summary\": \"What is the summary of the transcript?\",\n",
    "    \"topic\": \"Describe the topic of this transcript in one word\",\n",
    "    \"resolution\": \"Was the issue resolved? Answer in Yes or No\",\n",
    "    \"root_cause\": \"What was the root cause of the transcript? Write it in one line\",\n",
    "    \"call_back\": \"Does the agent need to call back the customer? Answer in Yes or No\",\n",
    "    \"next_steps\": \"What are the next steps?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a485826",
   "metadata": {},
   "source": [
    "## Generate Summaries for Each Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01f051",
   "metadata": {},
   "source": [
    "This section demonstrates how to generate the genAI powered fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35918ada-b2df-4388-99bb-b1c08838d490",
   "metadata": {},
   "source": [
    "Define task-specific function to generate the answer. You can change parameters like temperature, Top K, Top P as per the requirements. You can also alter the value of \"use_streaming\" to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68906794-3f56-43d0-967a-327e48603fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(\n",
    "    model_id,\n",
    "    user_prompt_template,\n",
    "    transcript,\n",
    "    field,\n",
    "    question,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.0,\n",
    "):\n",
    "\n",
    "    user_prompt = user_prompt_template.format(\n",
    "        TRANSCRIPT=transcript, FIELD=field, QUESTION=question\n",
    "    )\n",
    "    generated_text = None\n",
    "    if model_id.startswith(\"gpt-\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"},\n",
    "        ]\n",
    "        output_obj = invoke_openai_base(openai_client, messages, model_id)\n",
    "        generated_text = output_obj[\"response_text\"]\n",
    "    else:\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}]\n",
    "\n",
    "        output_obj = invoke_base(\n",
    "            client,\n",
    "            messages=messages,\n",
    "            system=None,\n",
    "            model_id=model_id,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=None,\n",
    "            top_p=None,\n",
    "            stop_sequences=[\"Human:\"],\n",
    "            use_streaming=False,\n",
    "            print_details=False,\n",
    "        )\n",
    "\n",
    "        generated_text = output_obj[\"response_text\"]\n",
    "\n",
    "    generated_text = generated_text.split(\"Step by Step Reason:\")[0]\n",
    "    generated_text = generated_text.replace(\"```\", \"\").strip(\"Answer: \").strip()\n",
    "\n",
    "    return (generated_text, output_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768f4a8-9716-40c0-b6d8-efa21127d7ab",
   "metadata": {},
   "source": [
    "### Specify model and generate answer for each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d32d1d6-1e77-4bc8-8613-5c83e3b2e1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model:mistral\n",
      "call summary:1\n",
      "  run prompt:raw\n",
      "    run field:summary\n",
      "    run field:topic\n",
      "    run field:resolution\n",
      "    run field:root cause\n",
      "    run field:call back\n",
      "    run field:next steps\n",
      "call summary:2\n",
      "  run prompt:raw\n",
      "    run field:summary\n",
      "    run field:topic\n",
      "    run field:resolution\n",
      "    run field:root cause\n",
      "    run field:call back\n",
      "    run field:next steps\n",
      "call summary:3\n",
      "  run prompt:raw\n",
      "    run field:summary\n",
      "    run field:topic\n",
      "    run field:resolution\n",
      "    run field:root cause\n",
      "    run field:call back\n",
      "    run field:next steps\n",
      "call summary:4\n",
      "  run prompt:raw\n",
      "    run field:summary\n",
      "    run field:topic\n",
      "    run field:resolution\n",
      "    run field:root cause\n",
      "    run field:call back\n",
      "    run field:next steps\n",
      "call summary:5\n",
      "  run prompt:raw\n",
      "    run field:summary\n",
      "    run field:topic\n",
      "    run field:resolution\n",
      "    run field:root cause\n",
      "    run field:call back\n",
      "    run field:next steps\n",
      "CPU times: user 152 ms, sys: 11.4 ms, total: 164 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = \"\"\n",
    "if model_id.startswith(\"gpt-\"):\n",
    "    model_name=\"openai\"\n",
    "else:\n",
    "    model_name=model_id.split(\".\")[0]\n",
    "final_file_name = f\"call_summarization_outputs_{model_name}.csv\"\n",
    "\n",
    "print(\"running model:%s\" % model_name)\n",
    "\n",
    "max_tokens = 256\n",
    "temperature = 0.0\n",
    "\n",
    "tList=[]\n",
    "n=0\n",
    "for x,y in transcripts.iterrows():\n",
    "    n=n+1\n",
    "    print(\"call summary:%s\" % n)\n",
    "    for prompt in prompt_dict:\n",
    "        if prompt==prompt_id:\n",
    "            row=y.copy()\n",
    "            user_prompt_template=prompt_dict[prompt]['prompt_text'] \n",
    "            print(\"  run prompt:%s\" % (prompt))\n",
    "            row['prompt_id']=prompt\n",
    "            for key in field_to_question_map.keys():\n",
    "                field = key.replace('_',' ')\n",
    "                print(\"    run field:%s\" % field)\n",
    "                question = field_to_question_map[key]\n",
    "                (genText, outputObj) = generate_answer(model_id, user_prompt_template, \n",
    "                    transcript = row['transcript'], field = field, question = question, \n",
    "                    max_tokens = max_tokens, temperature = temperature)\n",
    "                row[key]=genText\n",
    "                if field=='summary':\n",
    "                    row['metric_summary_input_tokens']=outputObj['input_tokens']\n",
    "                    row['metric_summary_output_tokens']=outputObj['output_tokens']\n",
    "                    row['metric_summary_output_tokens']=outputObj['output_tokens']\n",
    "                    row['metric_summary_latency']=outputObj['latency_end']\n",
    "            tList.append(row)\n",
    "transcripts=pd.DataFrame.from_records(tList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11398bc",
   "metadata": {},
   "source": [
    "Perform any post-processing of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebd36c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcripts = transcripts.loc[~transcripts.astype(str).eq(\"\").any(axis=1)]\n",
    "transcripts = transcripts.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1910152-d362-4055-b8f2-e15cc31da7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>call_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>date</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>metric_summary_input_tokens</th>\n",
       "      <th>metric_summary_output_tokens</th>\n",
       "      <th>metric_summary_latency</th>\n",
       "      <th>topic</th>\n",
       "      <th>resolution</th>\n",
       "      <th>root_cause</th>\n",
       "      <th>call_back</th>\n",
       "      <th>next_steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C101</td>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>3/5/24</td>\n",
       "      <td>raw</td>\n",
       "      <td>Sarah, the customer, called SB Bank to inquire...</td>\n",
       "      <td>767</td>\n",
       "      <td>150</td>\n",
       "      <td>4.195616</td>\n",
       "      <td>Credit-Card</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The root cause of the call was the customer's ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Sarah needs to wait for 7-10 business days to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C102</td>\n",
       "      <td>2</td>\n",
       "      <td>A02</td>\n",
       "      <td>Agent: Good morning, thank you for calling SB ...</td>\n",
       "      <td>3/11/24</td>\n",
       "      <td>raw</td>\n",
       "      <td>Sarah Thompson, a customer who applied for a c...</td>\n",
       "      <td>635</td>\n",
       "      <td>184</td>\n",
       "      <td>4.976691</td>\n",
       "      <td>Credit card delivery</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The root cause of the issue was the credit car...</td>\n",
       "      <td>No</td>\n",
       "      <td>Sarah will receive a replacement credit card w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C103</td>\n",
       "      <td>3</td>\n",
       "      <td>A03</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>2/29/24</td>\n",
       "      <td>raw</td>\n",
       "      <td>The customer, Sarah, was affected by recent fl...</td>\n",
       "      <td>676</td>\n",
       "      <td>121</td>\n",
       "      <td>3.438767</td>\n",
       "      <td>Extension</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The root cause was the recent floods in Califo...</td>\n",
       "      <td>No</td>\n",
       "      <td>The next step is for Sarah to make her minimum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C104</td>\n",
       "      <td>4</td>\n",
       "      <td>A04</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>3/7/24</td>\n",
       "      <td>raw</td>\n",
       "      <td>The customer, Sarah, was incorrectly charged a...</td>\n",
       "      <td>680</td>\n",
       "      <td>154</td>\n",
       "      <td>4.216477</td>\n",
       "      <td>Refund</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The root cause of the issue was a temporary sy...</td>\n",
       "      <td>No</td>\n",
       "      <td>The next steps are for the customer to wait fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C105</td>\n",
       "      <td>5</td>\n",
       "      <td>A05</td>\n",
       "      <td>\\nAgent: Good morning, thank you for calling S...</td>\n",
       "      <td>2/21/24</td>\n",
       "      <td>raw</td>\n",
       "      <td>Sarah Thompson reported a fraudulent transacti...</td>\n",
       "      <td>850</td>\n",
       "      <td>155</td>\n",
       "      <td>4.327091</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The root cause of the call was a fraudulent ai...</td>\n",
       "      <td>No</td>\n",
       "      <td>The next steps are for the agent to pass the d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  call_id agent_id  \\\n",
       "0        C101        1      A01   \n",
       "1        C102        2      A02   \n",
       "2        C103        3      A03   \n",
       "3        C104        4      A04   \n",
       "4        C105        5      A05   \n",
       "\n",
       "                                          transcript     date prompt_id  \\\n",
       "0  \\nAgent: Good morning, thank you for calling S...   3/5/24       raw   \n",
       "1  Agent: Good morning, thank you for calling SB ...  3/11/24       raw   \n",
       "2  \\nAgent: Good morning, thank you for calling S...  2/29/24       raw   \n",
       "3  \\nAgent: Good morning, thank you for calling S...   3/7/24       raw   \n",
       "4  \\nAgent: Good morning, thank you for calling S...  2/21/24       raw   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Sarah, the customer, called SB Bank to inquire...   \n",
       "1  Sarah Thompson, a customer who applied for a c...   \n",
       "2  The customer, Sarah, was affected by recent fl...   \n",
       "3  The customer, Sarah, was incorrectly charged a...   \n",
       "4  Sarah Thompson reported a fraudulent transacti...   \n",
       "\n",
       "   metric_summary_input_tokens  metric_summary_output_tokens  \\\n",
       "0                          767                           150   \n",
       "1                          635                           184   \n",
       "2                          676                           121   \n",
       "3                          680                           154   \n",
       "4                          850                           155   \n",
       "\n",
       "   metric_summary_latency                 topic resolution  \\\n",
       "0                4.195616           Credit-Card        Yes   \n",
       "1                4.976691  Credit card delivery        Yes   \n",
       "2                3.438767             Extension        Yes   \n",
       "3                4.216477                Refund        Yes   \n",
       "4                4.327091                 Fraud        Yes   \n",
       "\n",
       "                                          root_cause call_back  \\\n",
       "0  The root cause of the call was the customer's ...        No   \n",
       "1  The root cause of the issue was the credit car...        No   \n",
       "2  The root cause was the recent floods in Califo...        No   \n",
       "3  The root cause of the issue was a temporary sy...        No   \n",
       "4  The root cause of the call was a fraudulent ai...        No   \n",
       "\n",
       "                                          next_steps  \n",
       "0  Sarah needs to wait for 7-10 business days to ...  \n",
       "1  Sarah will receive a replacement credit card w...  \n",
       "2  The next step is for Sarah to make her minimum...  \n",
       "3  The next steps are for the customer to wait fo...  \n",
       "4  The next steps are for the agent to pass the d...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa2b76-918e-4e0b-9e47-808089aa4dac",
   "metadata": {},
   "source": [
    "Save results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc12d7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcripts.to_csv(f\"../outputs/{final_file_name}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
