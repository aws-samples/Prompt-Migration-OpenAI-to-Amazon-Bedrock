{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de2ff35-4352-4168-a306-67d448ea5bfd",
   "metadata": {},
   "source": [
    "# Evaluation with RAGAS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1149512-963a-4cd8-96f2-271fcfc61a21",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "1. The ragas-evaluation package is in the \"./use-case-examples/rag-investment-analysis-assistant/libraries/ragas-evaluation\" directory. \n",
    "2. Run `pip install -e . --quiet` in  the RAGAS package directory \"./use-case-examples/rag-investment-analysis-assistant/libraries/ragas-evaluation\" \n",
    "3. Install LlamaIndex, run `pip install llama-index==0.9.6.post1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f5ccf-4248-468d-a941-e1b0f9be642f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load packages and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dac98e7-0426-443c-ae56-b120f99385ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbee42e-7815-4b11-ba46-56a116b77418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "from ragas.metrics.answer_precision import AnswerPrecision, answer_precision\n",
    "from ragas.metrics.answer_recall import AnswerRecall, answer_recall\n",
    "from ragas.metrics.answer_correctness import AnswerCorrectness, answer_correctness\n",
    "from ragas.metrics.answer_relevance import AnswerRelevancy, answer_relevancy\n",
    "from ragas.metrics.answer_similarity import AnswerSimilarity, answer_similarity\n",
    "from ragas.metrics.context_precision import (\n",
    "    ContextPrecision,\n",
    "    ContextRelevancy,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas.metrics.context_recall import ContextRecall, context_recall\n",
    "from ragas.metrics.critique import AspectCritique\n",
    "from ragas.metrics.faithfulness import Faithfulness, faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837289dd-98aa-40a0-a878-aad01879821c",
   "metadata": {},
   "source": [
    "## Load and pre-process the samples for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f2c5b-f5d6-40ed-8020-31c4b6db8cd1",
   "metadata": {},
   "source": [
    "Load the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1efb9a-559e-43ef-bbb0-801c2d70a55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_folder = \"path/to/your/result/data/folder\"\n",
    "result_csv_file = result_folder+'sample_results.csv'\n",
    "\n",
    "result_df = pd.read_csv(result_csv_file, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788e9a0-8f3a-43bc-a929-b51a7df2b9b7",
   "metadata": {},
   "source": [
    "Pre-process the samples: This step depends on the format of the input samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ea5661-02fb-472b-a278-57da6296305e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure the \"contexts\" field is a list\n",
    "result_df['llm_contexts']=result_df['llm_contexts'].apply(lambda x: eval(x))\n",
    "\n",
    "# Ensure the \"ground_truths\" is field name for ground truth answer\n",
    "result_df.rename(columns={\"answer\":\"ground_truths\"}, inplace=True)\n",
    "\n",
    "# Ensure the \"ground_truths\" field is a list\n",
    "result_df['ground_truths']=result_df['ground_truths'].apply(lambda x: [x])\n",
    "\n",
    "# Ensure the \"llm_answer\" field has no None type\n",
    "result_df[[\"llm_answer\"]]=result_df[[\"llm_answer\"]].fillna(value=\"Unfortunately, I cannot answer this question\")\n",
    "\n",
    "result_ds = Dataset.from_pandas(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b628665-028e-4832-aba7-95ef40eb1888",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run ragas evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a41d86c-9606-4476-bf1b-6ffe049cac4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:18<00:00, 138.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:18<00:00, 78.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_correctness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:24<00:00, 144.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 1.25 s, total: 3.51 s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NOTE: Comment out any metrics you don't want to use\n",
    "metrics = [\n",
    "    answer_precision,\n",
    "    answer_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    # answer_relevancy,\n",
    "    # faithfulness,\n",
    "    # context_precision,\n",
    "    # context_recall, # currently this metric might trigger timeout error raised by bedrock: ValueError: Error raised by bedrock service: Read timeout on endpoint URL: \"https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-v2/invoke\"\n",
    "]\n",
    "\n",
    "column_map = {\n",
    "        \"question\": \"question\",\n",
    "        \"contexts\": \"llm_contexts\",\n",
    "        \"answer\": \"llm_answer\",\n",
    "        \"ground_truths\": \"ground_truths\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "eval_result = evaluate(result_ds, metrics=metrics, column_map=column_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810582b-4f69-4c51-bba1-dd4378edf9dc",
   "metadata": {},
   "source": [
    "## Save the evaluation metrics along with the input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfcb387-9c4c-4498-ab1e-8ea2f19b6f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the fields from the input datafram to the evaluation result dataframe\n",
    "eval_result_df = eval_result.to_pandas()\n",
    "metrics_keys = ['answer_precision','answer_recall','answer_correctness','answer_similarity']\n",
    "\n",
    "eval_result_df_new = result_df.merge(eval_result_df[metrics_keys], \n",
    "                                     how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a616b7a-c873-499c-8975-c14c4ad889bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save evaluation results to: ../data/results/sample_results_eval.csv\n"
     ]
    }
   ],
   "source": [
    "eval_result_csv_file = result_csv_file[:-4]+'_eval.csv'\n",
    "\n",
    "eval_result_df_new.to_csv(eval_result_csv_file, index=False)\n",
    "print(f\"Save evaluation results to: {eval_result_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048f069-fb13-46c5-9c44-30f37e641197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
