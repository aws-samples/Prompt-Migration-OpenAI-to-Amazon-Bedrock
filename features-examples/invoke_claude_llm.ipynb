{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3251d54b-adce-49e1-8dae-feacf213b545",
   "metadata": {},
   "source": [
    "# Anthropic Claude LLMs Invokation Guide\n",
    "\n",
    "This notebook walks you through how to call Anthropic's Claude LLMs through [Amazon Bedrock](https://aws.amazon.com/bedrock/). It includes the following examples:\n",
    "- Basic invocation\n",
    "- Invocation in streaming fashion\n",
    "- Invocation with printing invocation and latency detials\n",
    "- Invocation with JSON output\n",
    "- Invocation with batch processing and management\n",
    "\n",
    "This notebook requires Claude v3 Sonnet to be enabled in Bedrock via Model Access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3afe09d-7266-432e-9327-961e6381506e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "import boto3\n",
    "from typing import List\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from botocore.config import Config\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe254bd-7efe-49d1-b207-f6aa6921a1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Amazon Bedrock runtime client\n",
    "my_config = Config(\n",
    "    region_name = 'us-east-1',\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 3,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", config = my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b565434-3b9f-4912-a846-58f8d8add07c",
   "metadata": {},
   "source": [
    "## Claude invokation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a0cb82-8a99-432e-8732-82df0d72382c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_claude_base(client, \n",
    "                       messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello!\"}]}],\n",
    "                       system = \"You are an assistant.\",\n",
    "                       model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", \n",
    "                       max_tokens=1024, \n",
    "                       temperature = 1.0, \n",
    "                       top_k = None, \n",
    "                       top_p = None,\n",
    "                       stop_sequences=[\"Human:\"],\n",
    "                       use_streaming = False,\n",
    "                       anthropic_version = \"bedrock-2023-05-31\",\n",
    "                       print_details = True):\n",
    "    \"\"\"\n",
    "    Invokes Anthropic Claude models to run an inference using the input\n",
    "    provided in the request body.\n",
    "\n",
    "    :param prompt: The prompt that you want Claude 3 to complete.\n",
    "    :return: Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke Claude models with the text prompt\n",
    "    \n",
    "    body = {\n",
    "        \"anthropic_version\": anthropic_version,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "    \n",
    "    if system is not None: \n",
    "        body[\"system\"]= system\n",
    "    if top_k is not None: \n",
    "        body[\"top_k\"]= top_k\n",
    "    if top_p is not None: \n",
    "        body[\"top_p\"]= top_p\n",
    "    if stop_sequences is not None:    \n",
    "        body[\"stop_sequences\"] = stop_sequences\n",
    "    \n",
    "    time0 = time.time()\n",
    "    if use_streaming:\n",
    "        response = client.invoke_model_with_response_stream(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "        )\n",
    "        stream = response.get(\"body\")\n",
    "        output_text = \"\"\n",
    "        la = True\n",
    "        if stream:\n",
    "            for event in stream:\n",
    "                chunk = event.get(\"chunk\")\n",
    "                if chunk:\n",
    "                    if la:\n",
    "                        start_time = time.time() - time0\n",
    "                        print(f\"Response(s):\")\n",
    "                        #print(f\"\\n**** Stream Start {start_time} ****\\n\")\n",
    "                        la = False\n",
    "                    chunk_obj = json.loads(chunk.get(\"bytes\").decode())\n",
    "                    #print(chunk_obj)\n",
    "                    if chunk_obj[\"type\"]==\"content_block_delta\":\n",
    "                        text = chunk_obj[\"delta\"][\"text\"]\n",
    "                        print(text, end=\"\")\n",
    "                        output_text = output_text + text\n",
    "                    if chunk_obj[\"type\"]==\"message_stop\":\n",
    "                        input_tokens = chunk_obj[\"amazon-bedrock-invocationMetrics\"][\"inputTokenCount\"]\n",
    "                        output_tokens = chunk_obj[\"amazon-bedrock-invocationMetrics\"][\"outputTokenCount\"]\n",
    "                        latency_start = chunk_obj[\"amazon-bedrock-invocationMetrics\"][\"firstByteLatency\"]/1000\n",
    "                        latency_end = chunk_obj[\"amazon-bedrock-invocationMetrics\"][\"invocationLatency\"]/1000\n",
    "        end_time = time.time() - time0\n",
    "        output_list = [output_text]\n",
    "        #print(f\"\\n**** Stream End {end_time} ****\\n\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "        )\n",
    "        end_time = time.time() - time0\n",
    "        latency_start = end_time\n",
    "        latency_end = end_time\n",
    "\n",
    "        # Process and print the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        input_tokens = result[\"usage\"][\"input_tokens\"]\n",
    "        output_tokens = result[\"usage\"][\"output_tokens\"]\n",
    "        output_list = result.get(\"content\", [])\n",
    "        output_text = \"\\n\".join([x[\"text\"] for x in output_list])\n",
    "        print(f\"Response(s):\")\n",
    "        print(output_text)\n",
    "\n",
    "    if print_details:\n",
    "        print(\"Latency details:\")\n",
    "        print(f\"- The streaming start latency is {latency_start} seconds.\")\n",
    "        print(f\"- The full invocation latency is {latency_end} seconds.\")\n",
    "\n",
    "        print(\"Invocation details:\")\n",
    "        print(f\"- The input length is {input_tokens} tokens.\")\n",
    "        print(f\"- The output length is {output_tokens} tokens.\")\n",
    "    \n",
    "    output_obj = {\n",
    "        \"response_text\": output_text,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"latency_start\": latency_start,\n",
    "        \"latency_end\": latency_end,\n",
    "    }\n",
    "\n",
    "    return output_obj\n",
    "\n",
    "\n",
    "def invoke_claude_with_text(client, prompt,\n",
    "                       model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", \n",
    "                       max_tokens=1024, \n",
    "                       temperature = 1.0, \n",
    "                       top_k = None, \n",
    "                       top_p = None,\n",
    "                       stop_sequences=[\"Human:\"],\n",
    "                       use_streaming = False,\n",
    "                       anthropic_version = \"bedrock-2023-05-31\",\n",
    "                       print_details = True):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "    system = None\n",
    "    \n",
    "    return invoke_claude_base(client, \n",
    "                       messages = messages,\n",
    "                       system = system,\n",
    "                       model_id=model_id, \n",
    "                       max_tokens=max_tokens, \n",
    "                       temperature = temperature, \n",
    "                       top_k = top_k, \n",
    "                       top_p = top_p,\n",
    "                       stop_sequences=stop_sequences,\n",
    "                       use_streaming = use_streaming,\n",
    "                       anthropic_version = anthropic_version,\n",
    "                       print_details = print_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703f8f8-ce82-4d17-8272-794b9e175d67",
   "metadata": {},
   "source": [
    "## Invocation Example - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62de1cde-2013-4922-9e9d-86d9b0060c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(s):\n",
      "A large language model (LLM) is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, specifically transformer-based neural networks, to learn patterns and relationships in the training data, allowing them to perform a wide range of natural language processing tasks.\n",
      "\n",
      "Some key characteristics of large language models include:\n",
      "\n",
      "1. Massive scale: LLMs are trained on enormous datasets, often comprising billions or even trillions of words from various sources such as websites, books, and articles. This massive scale allows them to capture a broad understanding of language and knowledge.\n",
      "\n",
      "2. Generative capabilities: LLMs can generate human-like text, including coherent paragraphs, stories, articles, and even code, based on the input or prompt provided.\n",
      "\n",
      "3. Versatility: LLMs can be adapted to various natural language tasks, such as text summarization, question answering, translation, and even creative writing, by fine-tuning the model on task-specific data.\n",
      "\n",
      "4. Contextual understanding: LLMs can understand and generate text that takes into account the broader context, allowing for more natural and coherent language generation.\n",
      "\n",
      "Some well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and Megatron-Turing NLG by Microsoft and NVIDIA.\n",
      "\n",
      "While LLMs have demonstrated impressive language capabilities, they also have limitations and potential risks, such as generating biased or harmful content, lacking true understanding or reasoning, and being vulnerable to adversarial attacks. Ongoing research and development aim to address these challenges and improve the safety, robustness, and interpretability of large language models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response_text': 'A large language model (LLM) is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, specifically transformer-based neural networks, to learn patterns and relationships in the training data, allowing them to perform a wide range of natural language processing tasks.\\n\\nSome key characteristics of large language models include:\\n\\n1. Massive scale: LLMs are trained on enormous datasets, often comprising billions or even trillions of words from various sources such as websites, books, and articles. This massive scale allows them to capture a broad understanding of language and knowledge.\\n\\n2. Generative capabilities: LLMs can generate human-like text, including coherent paragraphs, stories, articles, and even code, based on the input or prompt provided.\\n\\n3. Versatility: LLMs can be adapted to various natural language tasks, such as text summarization, question answering, translation, and even creative writing, by fine-tuning the model on task-specific data.\\n\\n4. Contextual understanding: LLMs can understand and generate text that takes into account the broader context, allowing for more natural and coherent language generation.\\n\\nSome well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and Megatron-Turing NLG by Microsoft and NVIDIA.\\n\\nWhile LLMs have demonstrated impressive language capabilities, they also have limitations and potential risks, such as generating biased or harmful content, lacking true understanding or reasoning, and being vulnerable to adversarial attacks. Ongoing research and development aim to address these challenges and improve the safety, robustness, and interpretability of large language models.',\n",
       " 'input_tokens': 14,\n",
       " 'output_tokens': 402,\n",
       " 'latency_start': 13.511181831359863,\n",
       " 'latency_end': 13.511181831359863}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_id = \"anthropic.claude-instant-v1\"\n",
    "#model_id = \"anthropic.claude-v2:1\"\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "prompt = \"What is Large Langue Model?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "\n",
    "output_obj = \\\n",
    "invoke_claude_base(client, \n",
    "                   messages,\n",
    "                   system = None,\n",
    "                   model_id=model_id, \n",
    "                   max_tokens=1024, \n",
    "                   temperature = 0.0, \n",
    "                   top_k = None, \n",
    "                   top_p = None,\n",
    "                   stop_sequences=[\"Human:\"],\n",
    "                   use_streaming = False,\n",
    "                   anthropic_version = \"bedrock-2023-05-31\",\n",
    "                   print_details = False)\n",
    "\n",
    "output_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3422083-8449-48a3-b59e-0e3db4af1046",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invocation Example - Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6629333d-9a6f-4c9d-8205-721e59f123e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(s):\n",
      "A large language model (LLM) is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, specifically transformer-based neural networks, to learn patterns and relationships in the training data, allowing them to perform a wide range of natural language processing tasks.\n",
      "\n",
      "Some key characteristics of large language models include:\n",
      "\n",
      "1. Massive scale: LLMs are trained on enormous datasets, often comprising billions or even trillions of words from various sources such as websites, books, and articles. This massive scale allows them to capture a broad understanding of language and knowledge.\n",
      "\n",
      "2. Generative capabilities: LLMs can generate human-like text, including coherent paragraphs, stories, articles, and even code, based on the input or prompt provided.\n",
      "\n",
      "3. Versatility: LLMs can be adapted to various natural language tasks, such as text summarization, question answering, translation, and even creative writing, by fine-tuning the model on task-specific data.\n",
      "\n",
      "4. Contextual understanding: LLMs can understand and generate text that takes into account the broader context, allowing for more natural and coherent language generation.\n",
      "\n",
      "Some well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and Megatron-Turing NLG by Microsoft and NVIDIA.\n",
      "\n",
      "While LLMs have demonstrated impressive language capabilities, they also have limitations and potential risks, such as generating biased or harmful content, lacking true understanding or reasoning abilities, and being susceptible to misuse or misrepresentation. Ongoing research and development aim to address these challenges and improve the safety, reliability, and transparency of large language models.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response_text': 'A large language model (LLM) is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, specifically transformer-based neural networks, to learn patterns and relationships in the training data, allowing them to perform a wide range of natural language processing tasks.\\n\\nSome key characteristics of large language models include:\\n\\n1. Massive scale: LLMs are trained on enormous datasets, often comprising billions or even trillions of words from various sources such as websites, books, and articles. This massive scale allows them to capture a broad understanding of language and knowledge.\\n\\n2. Generative capabilities: LLMs can generate human-like text, including coherent paragraphs, stories, articles, and even code, based on the input or prompt provided.\\n\\n3. Versatility: LLMs can be adapted to various natural language tasks, such as text summarization, question answering, translation, and even creative writing, by fine-tuning the model on task-specific data.\\n\\n4. Contextual understanding: LLMs can understand and generate text that takes into account the broader context, allowing for more natural and coherent language generation.\\n\\nSome well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and Megatron-Turing NLG by Microsoft and NVIDIA.\\n\\nWhile LLMs have demonstrated impressive language capabilities, they also have limitations and potential risks, such as generating biased or harmful content, lacking true understanding or reasoning abilities, and being susceptible to misuse or misrepresentation. Ongoing research and development aim to address these challenges and improve the safety, reliability, and transparency of large language models.',\n",
       " 'input_tokens': 14,\n",
       " 'output_tokens': 404,\n",
       " 'latency_start': 0.39,\n",
       " 'latency_end': 11.644}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_id = \"anthropic.claude-instant-v1\"\n",
    "#model_id = \"anthropic.claude-v2:1\"\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "prompt = \"What is Large Langue Model?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "\n",
    "output_obj = \\\n",
    "invoke_claude_base(client, \n",
    "                   messages,\n",
    "                   system = None,\n",
    "                   model_id=model_id, \n",
    "                   max_tokens=1024, \n",
    "                   temperature = 0.0, \n",
    "                   top_k = None, \n",
    "                   top_p = None,\n",
    "                   stop_sequences=[\"Human:\"],\n",
    "                   use_streaming = True,\n",
    "                   anthropic_version = \"bedrock-2023-05-31\",\n",
    "                   print_details = False)\n",
    "\n",
    "output_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adee380-c4e9-4e5a-99e8-763e8fa26dfe",
   "metadata": {},
   "source": [
    "## Invocation Example - Streaming with printing invocation and latency detials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0459bbd-a186-4ca9-a15f-9e97dca106f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(s):\n",
      "A large language model (LLM) is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, specifically transformer-based neural networks, to learn patterns and relationships in the training data, allowing them to perform a wide range of natural language processing tasks.\n",
      "\n",
      "Some key characteristics of large language models include:\n",
      "\n",
      "1. Massive scale: LLMs are trained on enormous datasets, often comprising billions or even trillions of words from various sources such as websites, books, and articles. This massive scale allows them to capture a broad understanding of language and knowledge.\n",
      "\n",
      "2. Generative capabilities: LLMs can generate human-like text, including coherent paragraphs, stories, articles, and even code, based on the input or prompt provided.\n",
      "\n",
      "3. Versatility: LLMs can be adapted to various natural language tasks, such as text generation, translation, summarization, question answering, and text classification, by fine-tuning the model on task-specific data.\n",
      "\n",
      "4. Contextual understanding: LLMs can understand and generate text that takes into account the broader context, allowing them to produce more coherent and relevant outputs.\n",
      "\n",
      "5. Open-ended knowledge: LLMs can leverage their training on broad datasets to demonstrate knowledge and reasoning abilities across a wide range of topics, although their knowledge can be inconsistent or biased based on the training data.\n",
      "\n",
      "Some well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and LaMDA (Language Model for Dialogue Applications) by Google.\n",
      "\n",
      "While LLMs have shown impressive capabilities, they also raise concerns about potential biases, factual inaccuracies, and the potential for misuse, leading to ongoing research and discussions around their responsible development and deployment.\n",
      "\n",
      "Latency details:\n",
      "- The streaming start latency is 0.346 seconds.\n",
      "- The full invocation latency is 15.391 seconds.\n",
      "Invocation details:\n",
      "- The input length is 14 tokens.\n",
      "- The output length is 425 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response_text': 'A large language model (LLM) is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, specifically transformer-based neural networks, to learn patterns and relationships in the training data, allowing them to perform a wide range of natural language processing tasks.\\n\\nSome key characteristics of large language models include:\\n\\n1. Massive scale: LLMs are trained on enormous datasets, often comprising billions or even trillions of words from various sources such as websites, books, and articles. This massive scale allows them to capture a broad understanding of language and knowledge.\\n\\n2. Generative capabilities: LLMs can generate human-like text, including coherent paragraphs, stories, articles, and even code, based on the input or prompt provided.\\n\\n3. Versatility: LLMs can be adapted to various natural language tasks, such as text generation, translation, summarization, question answering, and text classification, by fine-tuning the model on task-specific data.\\n\\n4. Contextual understanding: LLMs can understand and generate text that takes into account the broader context, allowing them to produce more coherent and relevant outputs.\\n\\n5. Open-ended knowledge: LLMs can leverage their training on broad datasets to demonstrate knowledge and reasoning abilities across a wide range of topics, although their knowledge can be inconsistent or biased based on the training data.\\n\\nSome well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI, BERT (Bidirectional Encoder Representations from Transformers) by Google, and LaMDA (Language Model for Dialogue Applications) by Google.\\n\\nWhile LLMs have shown impressive capabilities, they also raise concerns about potential biases, factual inaccuracies, and the potential for misuse, leading to ongoing research and discussions around their responsible development and deployment.',\n",
       " 'input_tokens': 14,\n",
       " 'output_tokens': 425,\n",
       " 'latency_start': 0.346,\n",
       " 'latency_end': 15.391}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_id = \"anthropic.claude-instant-v1\"\n",
    "#model_id = \"anthropic.claude-v2:1\"\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "prompt = \"What is Large Langue Model?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "\n",
    "output_obj = \\\n",
    "invoke_claude_base(client, \n",
    "                   messages,\n",
    "                   system = None,\n",
    "                   model_id=model_id, \n",
    "                   max_tokens=1024, \n",
    "                   temperature = 0.0, \n",
    "                   top_k = None, \n",
    "                   top_p = None,\n",
    "                   stop_sequences=[\"Human:\"],\n",
    "                   use_streaming = True,\n",
    "                   anthropic_version = \"bedrock-2023-05-31\",\n",
    "                   print_details = True)\n",
    "\n",
    "output_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9af85b-aad7-4983-8171-a5c30369eeab",
   "metadata": {},
   "source": [
    "## Invocation Example - JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e3bffda-f94d-45bd-ac3f-b53d6a48d443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_QUERY_META_GENERATION = \"\"\"\n",
    "        \\n\\nHuman:\n",
    "                Financial question related to yearly and Quarterly financial Reports: {query} \\n\n",
    "                Generate the keywords and rephrase the question to make it very clear, think step by step:\n",
    "\n",
    "                1. Expand any acronyms and abbreviations in the original question by providing the full term. Include both the original abbreviated version and the expanded version in the rephrased question.\n",
    "\n",
    "                2. If there is no time keywords mentioned in the original question, do not include time keywords in the rephrased question either.\n",
    "\n",
    "                3. Generate a list of company names that are mentioned in the question.\n",
    "\n",
    "                4. Generate a comprehensive list of all technical keywords and key phrases that are relevant to answering the question.\n",
    "\n",
    "                5. Pay close attention to any time spans requested in the original question, such as specific years, quarters, or months.\n",
    "\n",
    "                6. Generate a list of time_keywords using a 'Quarter Year' format (e.g. Q1'22). Include only the time keywords related to the question. If there is no time-related keywords mentioned in the original question, please leave this time_keywords as an empty list. Do not include the most recent (last) quarter in the time_keywords if it's not needed for answering the question.\n",
    "\n",
    "                7. Return a JSON object with the following fields:\n",
    "                   - 'time_keywords': a list of time-related keywords\n",
    "                   - 'technical_keywords': a list of technical keywords\n",
    "                   - 'company_keywords': a list of company names\n",
    "                   - 'rephrased_question': the full rephrased question string\n",
    "\n",
    "\n",
    "        \\n\\nAssistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2bce6f6-c593-4b1b-8205-306c2b0a6014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(s):\n",
      "{\n",
      "  \"time_keywords\": [\"2022\"],\n",
      "  \"technical_keywords\": [\"revenue growth\", \"product segments\"],\n",
      "  \"company_keywords\": [\"Apple\"],\n",
      "  \"rephrased_question\": \"What was the revenue growth for each of Apple's (Apple Inc.) product segments in the year 2022?\"\n",
      "}\n",
      "\n",
      "Latency details:\n",
      "- The streaming start latency is 0.657 seconds.\n",
      "- The full invocation latency is 3.136 seconds.\n",
      "Invocation details:\n",
      "- The input length is 377 tokens.\n",
      "- The output length is 75 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response_text': '{\\n  \"time_keywords\": [\"2022\"],\\n  \"technical_keywords\": [\"revenue growth\", \"product segments\"],\\n  \"company_keywords\": [\"Apple\"],\\n  \"rephrased_question\": \"What was the revenue growth for each of Apple\\'s (Apple Inc.) product segments in the year 2022?\"\\n}',\n",
       " 'input_tokens': 377,\n",
       " 'output_tokens': 75,\n",
       " 'latency_start': 0.657,\n",
       " 'latency_end': 3.136}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_id = \"anthropic.claude-instant-v1\"\n",
    "#model_id = \"anthropic.claude-v2:1\"\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "query = \"What was the revenue growth for each of Apple's product segments in 2022?\"\n",
    "prompt = PROMPT_QUERY_META_GENERATION.format(query = query)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "\n",
    "output_obj = \\\n",
    "invoke_claude_base(client, \n",
    "                   messages,\n",
    "                   system = None,\n",
    "                   model_id=model_id, \n",
    "                   max_tokens=1024, \n",
    "                   temperature = 0.0, \n",
    "                   top_k = None, \n",
    "                   top_p = None,\n",
    "                   stop_sequences=[\"Human:\"],\n",
    "                   use_streaming = True,\n",
    "                   anthropic_version = \"bedrock-2023-05-31\",\n",
    "                   print_details = True)\n",
    "\n",
    "output_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9a89d-05d4-486e-8031-e70d134f0a9c",
   "metadata": {},
   "source": [
    "## Invocation Example - Batch processing\n",
    "The code for this example is for demo only as the customer's specific data is not shareable in this codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a627200-8ae7-463b-b4e0-f50d6b21d4fd",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8e1a9-dd44-4154-8503-6e14e3938057",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/broadcast_segmentation_ds.xlsx\"\n",
    "df_data = pd.read_excel(data_file, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f848992-f406-407f-a403-1ffc0e7b35fa",
   "metadata": {},
   "source": [
    "Run the invocation of LLM and save the output results (JSON) for each sample in {output_folder_root}/{run1,run2,run3,...}\n",
    "\n",
    "Note: Please modify this cell for your own use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3ea2e-cf57-4ba3-91d5-9c354384fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "max_tokens=4096\n",
    "temperature=0.5\n",
    "top_k = None\n",
    "top_p = None\n",
    "input_content_field = \"trimmed_paragraph\"\n",
    "PROMPT_TEMPLATE = PROMPT_QA_v2\n",
    "use_streaming = True\n",
    "\n",
    "output_folder_root = \\\n",
    "\"../data/results/may07_claudev3s_trimmed_promptv2_tem05_stream_fullseg\"\n",
    "\n",
    "for run in ['run1', 'run2', 'run3']:\n",
    "    print(f\"##########{run}\")\n",
    "    output_folder = os.path.join(output_folder_root,run)\n",
    "\n",
    "    for idx in range(len(df_data)):\n",
    "        output_file_name = os.path.join(output_folder, f\"result_sample{idx}.json\")\n",
    "        if os.path.exists(output_file_name):\n",
    "            print(f\"###### Already Finished for Sample {idx}\")\n",
    "            continue         \n",
    "        else:    \n",
    "            row = df_data.iloc[idx]\n",
    "            prompt = PROMPT_TEMPLATE.format(keywords_list = [row[\"keyword\"]], input_content = row[input_content_field])\n",
    "            print(f\"###### Result for Sample {idx}\") \n",
    "            output_obj= invoke_claude_with_text(prompt, model_id, \n",
    "                                                max_tokens=max_tokens, temperature=temperature, top_k = top_k, top_p = top_p, use_streaming = use_streaming)\n",
    "\n",
    "        result, input_tokens, output_tokens, latency_start, latency_end =\\\n",
    "        output_obj[\"response_text\"], output_obj[\"input_tokens\"], output_obj[\"output_tokens\"], output_obj[\"latency_start\"], output_obj[\"latency_end\"]  \n",
    "\n",
    "        dict_temp = {\n",
    "            \"sample_id\": idx,\n",
    "            \"response\": result,\n",
    "            \"input_tokens\": input_tokens, \n",
    "            \"output_tokens\": output_tokens, \n",
    "            \"latency_start\": latency_start, \n",
    "            \"latency_end\": latency_end,\n",
    "        }\n",
    "\n",
    "        with open(output_file_name, 'w') as f:\n",
    "            json.dump(dict_temp, f)\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b0b1d-5dbf-4eef-8912-db41feaeb786",
   "metadata": {
    "tags": []
   },
   "source": [
    "Read the output results for each sample in {output_folder_root}/{run1,run2,run3,...}, and aggregate them into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897d44e-2379-4814-b96d-33c9aebda337",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_root = \\\n",
    "\"../data/results/may07_claudev3s_trimmed_promptv2_tem05_stream_fullseg\"\n",
    "\n",
    "runs = os.listdir(output_folder_root)\n",
    "runs = [x for x in runs if x.find('run')>=0]\n",
    "\n",
    "list_df = []\n",
    "for run in runs:\n",
    "    output_folder = os.path.join(output_folder_root,run)\n",
    "    list_result_files = os.listdir(output_folder)\n",
    "    list_result_files = [x for x in list_result_files if x.find('.json')>=0]\n",
    "\n",
    "    list_dict = []\n",
    "    for result_file in list_result_files:\n",
    "        with open(os.path.join(output_folder,result_file),'r') as f:\n",
    "            json_temp = json.load(f)\n",
    "            list_dict.append(json_temp)\n",
    "\n",
    "    df_results_temp = pd.DataFrame(list_dict)\n",
    "    df_results_temp = df_results_temp.sort_values('sample_id').reset_index()\n",
    "    df_results_temp['run'] = run\n",
    "    list_df.append(df_results_temp)\n",
    "    \n",
    "df_results_all = pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab30e1-e982-4b7b-92d7-d51a6624aa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f593fe-ae26-4626-be6a-11c01693797b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
